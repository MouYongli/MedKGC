{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RadGraph MIMIC-CXR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'end_ix': 36,\n",
      "       'label': 'ANAT-DP',\n",
      "       'relations': [],\n",
      "       'start_ix': 36,\n",
      "       'tokens': 'Lungs'},\n",
      " '2': {'end_ix': 38,\n",
      "       'label': 'OBS-DP',\n",
      "       'relations': [['located_at', '1']],\n",
      "       'start_ix': 38,\n",
      "       'tokens': 'clear'},\n",
      " '3': {'end_ix': 40,\n",
      "       'label': 'OBS-DP',\n",
      "       'relations': [['located_at', '4'],\n",
      "                     ['located_at', '5'],\n",
      "                     ['located_at', '7']],\n",
      "       'start_ix': 40,\n",
      "       'tokens': 'Normal'},\n",
      " '4': {'end_ix': 41,\n",
      "       'label': 'ANAT-DP',\n",
      "       'relations': [],\n",
      "       'start_ix': 41,\n",
      "       'tokens': 'cardiomediastinal'},\n",
      " '5': {'end_ix': 43,\n",
      "       'label': 'ANAT-DP',\n",
      "       'relations': [],\n",
      "       'start_ix': 43,\n",
      "       'tokens': 'hilar'},\n",
      " '6': {'end_ix': 44,\n",
      "       'label': 'ANAT-DP',\n",
      "       'relations': [['modify', '4'], ['modify', '5']],\n",
      "       'start_ix': 44,\n",
      "       'tokens': 'silhouettes'},\n",
      " '7': {'end_ix': 46,\n",
      "       'label': 'ANAT-DP',\n",
      "       'relations': [],\n",
      "       'start_ix': 46,\n",
      "       'tokens': 'pleural'},\n",
      " '8': {'end_ix': 47,\n",
      "       'label': 'ANAT-DP',\n",
      "       'relations': [['modify', '7']],\n",
      "       'start_ix': 47,\n",
      "       'tokens': 'surfaces'}}\n",
      "FINAL REPORT EXAMINATION : CHEST ( PORTABLE AP ) INDICATION : ___ year old woman with SAH / / Fever workup Fever workup IMPRESSION : Compared to chest radiographs ___ . Patient has been extubated . Lungs are clear . Normal cardiomediastinal and hilar silhouettes and pleural surfaces .\n",
      "12388\n",
      "1553\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "graph_train_path = '/DATA1/llm-research/RadGraph/physionet.org/files/radgraph/1.0.0/train.json'\n",
    "\n",
    "with open(graph_train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# pprint.pprint(train_data)\n",
    "\n",
    "entities = train_data['p18/p18004941/s58821758.txt']['entities']\n",
    "pprint.pprint(entities)\n",
    "\n",
    "# print text\n",
    "print(train_data['p18/p18004941/s58821758.txt']['text'])\n",
    "\n",
    "\n",
    "# Step 1: Initialize an empty list for tokens\n",
    "all_tokens = []\n",
    "\n",
    "# Step 2: Iterate through each document in train_data\n",
    "for document_key in train_data:\n",
    "    # Access the 'entities' for the current document\n",
    "    entities = train_data[document_key]['entities']\n",
    "    \n",
    "    # Step 4: Iterate through each entity in 'entities'\n",
    "    for entity_key in entities:\n",
    "        # Step 5: Access the 'tokens' field and append its value to the all_tokens list\n",
    "        all_tokens.append(entities[entity_key]['tokens'])\n",
    "\n",
    "# At this point, all_tokens list contains all tokens from the entire train_data\n",
    "print(len(all_tokens))\n",
    "# print(all_tokens)\n",
    "\n",
    "# Step 1: Initialize an empty set for tokens\n",
    "all_tokens = set()\n",
    "\n",
    "# Step 2: Iterate through each document in train_data\n",
    "for document_key in train_data:\n",
    "    # Access the 'entities' for the current document\n",
    "    entities = train_data[document_key]['entities']\n",
    "    \n",
    "    # Step 4: Iterate through each entity in 'entities'\n",
    "    for entity_key in entities:\n",
    "        # Step 5: Access the 'tokens' field and add its value to the all_tokens set with 'label', as tupel\n",
    "        all_tokens.add((entities[entity_key]['tokens'], entities[entity_key]['label']))\n",
    "\n",
    "# Convert the set back to a list if you need a list format\n",
    "unique_tokens_list = list(all_tokens)\n",
    "\n",
    "# At this point, unique_tokens_list contains all unique tokens from the entire train_data\n",
    "print(len(unique_tokens_list))\n",
    "# print(unique_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OBS-U', 'OBS-DA', 'OBS-DP', 'ANAT-DP'}\n",
      "4\n",
      "{'OBS-DP': 812, 'ANAT-DP': 398, 'OBS-U': 190, 'OBS-DA': 153}\n"
     ]
    }
   ],
   "source": [
    "unique_second_elements = {token[1] for token in all_tokens}\n",
    "print(unique_second_elements)\n",
    "num_unique_second_elements = len(unique_second_elements)\n",
    "print(num_unique_second_elements)\n",
    "\n",
    "type_counts = {}\n",
    "for token in all_tokens:\n",
    "    token_type = token[1]\n",
    "    if token_type in type_counts:\n",
    "        type_counts[token_type] += 1\n",
    "    else:\n",
    "        type_counts[token_type] = 1\n",
    "\n",
    "print(type_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entity: 12388  \n",
    "unique: 1250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = set()\n",
    "\n",
    "# Step 1: Iterate through each document in train_data\n",
    "for document_key in train_data:\n",
    "    # Access the 'entities' for the current document\n",
    "    entities = train_data[document_key]['entities']\n",
    "    \n",
    "    # Step 2: Iterate through each entity in 'entities'\n",
    "    for entity_key in entities:\n",
    "        # Access the entity and its relations\n",
    "        entity = entities[entity_key]\n",
    "        entity_relations = entity['relations']\n",
    "        \n",
    "        # Step 3: Iterate through each relation in 'entity_relations'\n",
    "        for relation in entity_relations:\n",
    "            # Extract the related entity key and relation type\n",
    "            related_entity_key = relation[1]\n",
    "            relation_type = relation[0]\n",
    "            \n",
    "            # Get the related entity\n",
    "            related_entity = entities[related_entity_key]\n",
    "            \n",
    "            # Create a tuple with entity1, entity2, and relation\n",
    "            relation_tuple = (entity['tokens'], related_entity['tokens'], relation_type)\n",
    "            \n",
    "            # Add the relation tuple to the set of relations\n",
    "            relations.add(relation_tuple)\n",
    "\n",
    "# At this point, 'relations' contains all the relations between entities in the 'train_data'\n",
    "print(len(relations))\n",
    "print(relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3300 relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "\n",
    "# 连接到Neo4j数据库\n",
    "graph = Graph(\"bolt://localhost:7689\", auth=(\"neo4j\", \"neo4j\"))\n",
    "\n",
    "# 创建节点\n",
    "# alice = Node(\"Person\", name=\"Alice\", age=25)\n",
    "# bob = Node(\"Person\", name=\"Bob\", age=30)\n",
    "# graph.create(alice)\n",
    "# graph.create(bob)\n",
    "\n",
    "# # 创建关系\n",
    "# alice_knows_bob = Relationship(alice, \"KNOWS\", bob)\n",
    "# graph.create(alice_knows_bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "def get_bert_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    return last_hidden_states\n",
    "\n",
    "\n",
    "def get_cls_embedding(text, model, tokenizer):\n",
    "    embeddings = get_bert_embedding(text, model, tokenizer)\n",
    "    cls_embedding = embeddings[0, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "def get_first_word_embedding(text, model, tokenizer):\n",
    "    embeddings = get_bert_embedding(text, model, tokenizer)\n",
    "    first_word = text.split(' is ')[0]\n",
    "    tokens = tokenizer.tokenize(first_word)\n",
    "    word_embeddings = embeddings[0, 1:len(tokens)+1, :]\n",
    "    mean_embedding = torch.mean(word_embeddings, dim=0)\n",
    "    return mean_embedding\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Pneumothorax is An accumulation of air or gas in the PLEURAL CAVITY, which may occur spontaneously or as a result of trauma or a pathological process. The gas may also be introduced deliberately during PNEUMOTHORAX, ARTIFICIAL.\"\n",
    "\n",
    "cls_embedding = get_cls_embedding(text, model, tokenizer)\n",
    "print(f\"CLS Embedding shape: {cls_embedding.shape}\")\n",
    "\n",
    "first_word_embedding = get_first_word_embedding(text, model, tokenizer)  \n",
    "print(f\"First Word Embedding shape: {first_word_embedding.shape}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def find_most_similar_item(mention_embedding, embeddings_array, items):\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(mention_embedding, embeddings_array)\n",
    "\n",
    "    # Print the cosine similarity for each item\n",
    "    for i, similarity in enumerate(similarities[0]):\n",
    "        print(f\"Cosine similarity with item {i+1}: {similarity:.4f}\")\n",
    "\n",
    "    # Find the index of the most similar item\n",
    "    most_similar_index = similarities.argmax()\n",
    "    print(f\"\\nMost similar item: \\n{items[most_similar_index]}\")\n",
    "\n",
    "def find_nearest_item(mention_embedding, embeddings_array, items):\n",
    "    # Calculate the Euclidean distance between mention_embedding and each embedding in embeddings_array\n",
    "    distances = np.linalg.norm(mention_embedding - embeddings_array, axis=1)\n",
    "\n",
    "    # Find the index of the nearest embedding\n",
    "    nearest_index = np.argmin(distances)\n",
    "\n",
    "    # Print the nearest item\n",
    "    nearest_item = items[nearest_index]\n",
    "    print(f\"\\nNearest item of Euclidean distance: \\n{nearest_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the items\n",
    "item1 = \"Pneumothorax is An accumulation of air or gas in the PLEURAL CAVITY, which may occur spontaneously or as a result of trauma or a pathological process. The gas may also be introduced deliberately during PNEUMOTHORAX, ARTIFICIAL. (MSH)\"\n",
    "item2 = \"Lungs is Either of the pair of organs occupying the cavity of the thorax that effect the aeration of the blood. (MSH)\"\n",
    "item3 = \"Pleural is Of or pertaining to the pleura. (NCI)\"\n",
    "item4 = \"Thyroidectomy is Surgical removal of the thyroid gland.\" \n",
    "items = [item1, item2, item3, item4]\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "model_name = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Encode the items using the CLS embedding\n",
    "# embeddings = [get_first_word_embedding(item, model, tokenizer) for item in items]\n",
    "embeddings = [get_cls_embedding(item, model, tokenizer) for item in items]\n",
    "embeddings_array = torch.stack(embeddings).detach().numpy()\n",
    "\n",
    "\n",
    "# Get embedding for the mention\n",
    "mention = \"Pneumothorax\"  \n",
    "mention_embedding = get_cls_embedding(mention, model, tokenizer).detach().numpy().reshape(1, -1)\n",
    "\n",
    "print(f\"Mention: {mention}\\n\")\n",
    "find_most_similar_item(mention_embedding, embeddings_array, items)\n",
    "find_nearest_item(mention_embedding, embeddings_array, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for the mention\n",
    "mention = \"lung\"  \n",
    "mention_embedding = get_cls_embedding(mention, model, tokenizer).detach().numpy().reshape(1, -1)\n",
    "\n",
    "print(f\"Mention: {mention}\\n\")\n",
    "find_most_similar_item(mention_embedding, embeddings_array, items)\n",
    "find_nearest_item(mention_embedding, embeddings_array, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for the mention\n",
    "mention = \"Pneumothorax is the buildup of air or gas in the pleural cavity, which can happen spontaneously, due to trauma, or because of a disease. The gas can also be intentionally introduced during artificial pneumothorax.\"  \n",
    "mention_embedding = get_cls_embedding(mention, model, tokenizer).detach().numpy().reshape(1, -1)\n",
    "\n",
    "print(f\"Mention: {mention}\\n\")\n",
    "find_most_similar_item(mention_embedding, embeddings_array, items)\n",
    "find_nearest_item(mention_embedding, embeddings_array, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for the mention\n",
    "mention = \"PLungs are one of the pair of organs located in the thoracic cavity that facilitate the aeration of blood.\"  \n",
    "mention_embedding = get_cls_embedding(mention, model, tokenizer).detach().numpy().reshape(1, -1)\n",
    "\n",
    "print(f\"Mention: {mention}\\n\")\n",
    "find_most_similar_item(mention_embedding, embeddings_array, items)\n",
    "find_nearest_item(mention_embedding, embeddings_array, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "# 示例文本\n",
    "text = \"\"\"苹果公司是一家科技巨头，总部位于加利福尼亚。它以iPhone和Mac电脑而闻名。\n",
    "苹果公司在全球拥有众多粉丝，产品设计备受赞誉。微软是另一家主要的科技公司，\n",
    "专注于软件开发。两家公司都在AI领域有重大投资。谷歌也是AI领域的重要玩家，\n",
    "与苹果公司在多个领域展开竞争。\"\"\"\n",
    "\n",
    "main_entity = \"苹果公司\"\n",
    "related_entities = [\"微软\", \"谷歌\"]\n",
    "attributes = [\"科技\", \"创新\", \"产品\"]\n",
    "\n",
    "# 方法1: 实体中心化嵌入\n",
    "def entity_centric_embedding(text, entity):\n",
    "    return f\"关于{entity}的信息：{text}\"\n",
    "\n",
    "# 方法2: 相关句子提取嵌入\n",
    "def relevant_sentence_embedding(text, entity):\n",
    "    sentences = sent_tokenize(text)\n",
    "    relevant_sentences = [s for s in sentences if entity in s]\n",
    "    return f\"{entity}: {' '.join(relevant_sentences)}\"\n",
    "\n",
    "# 方法3: 实体上下文窗口嵌入\n",
    "def context_window_embedding(text, entity, window_size=1):\n",
    "    sentences = sent_tokenize(text)\n",
    "    relevant_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if entity in sentence:\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sentences), i + window_size + 1)\n",
    "            relevant_sentences.extend(sentences[start:end])\n",
    "    return f\"{entity} 上下文: {' '.join(relevant_sentences)}\"\n",
    "\n",
    "# 方法4: 实体属性强化嵌入\n",
    "def entity_attribute_embedding(text, entity, attributes):\n",
    "    pattern = r'\\b' + re.escape(entity) + r'\\b.{0,50}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    attribute_text = \" \".join([f\"{entity} {attr}\" for attr in attributes])\n",
    "    context = \" \".join(matches)\n",
    "    return f\"{attribute_text}\\n实体上下文: {context}\"\n",
    "\n",
    "# 方法5: 实体关系焦点嵌入\n",
    "def entity_relation_embedding(text, main_entity, related_entities):\n",
    "    sentences = sent_tokenize(text)\n",
    "    relevant_sentences = [s for s in sentences if main_entity in s and any(e in s for e in related_entities)]\n",
    "    return f\"{main_entity}与其他实体的关系: {' '.join(relevant_sentences)}\"\n",
    "\n",
    "# 打印每种方法的结果\n",
    "print(\"方法1: 实体中心化嵌入\")\n",
    "print(entity_centric_embedding(text, main_entity))\n",
    "print(\"\\n方法2: 相关句子提取嵌入\")\n",
    "print(relevant_sentence_embedding(text, main_entity))\n",
    "print(\"\\n方法3: 实体上下文窗口嵌入\")\n",
    "print(context_window_embedding(text, main_entity))\n",
    "print(\"\\n方法4: 实体属性强化嵌入\")\n",
    "print(entity_attribute_embedding(text, main_entity, attributes))\n",
    "print(\"\\n方法5: 实体关系焦点嵌入\")\n",
    "print(entity_relation_embedding(text, main_entity, related_entities))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medkgc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
