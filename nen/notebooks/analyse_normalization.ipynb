{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file all_unique_entities.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 1250\n",
      "First 5 items:\n",
      "{'Lungs': {'label': 'ANAT-DP',\n",
      "           'normalization': {'UMLS': {'definition': {'definition': 'Lobular organ the parenchyma '\n",
      "                                                                   'of which consists of '\n",
      "                                                                   'air-filled alveoli which '\n",
      "                                                                   'communicate with the '\n",
      "                                                                   'tracheobronchial tree. '\n",
      "                                                                   'Examples: There are only two '\n",
      "                                                                   'instances, right lung and left '\n",
      "                                                                   'lung.',\n",
      "                                                     'source': 'UWDA'},\n",
      "                                      'name': 'Lungs',\n",
      "                                      'semanticTypes': 'Body Part, Organ, or Organ Component',\n",
      "                                      'ui': 'C0024109'}},\n",
      "           'reports': [{'p18/p18004941/s58821758.txt': {'end_ix': 36, 'start_ix': 36}},\n",
      "                       {'p18/p18003081/s53302126.txt': {'end_ix': 52, 'start_ix': 52}},\n",
      "                       {'p18/p18001922/s52638004.txt': {'end_ix': 46, 'start_ix': 46}},\n",
      "                       {'p18/p18001922/s52288833.txt': {'end_ix': 52, 'start_ix': 52}},\n",
      "                       {'p15/p15165563/s51659523.txt': {'end_ix': 110, 'start_ix': 110}},\n",
      "                       {'p15/p15004061/s56845046.txt': {'end_ix': 42, 'start_ix': 42}},\n",
      "                       {'p15/p15078112/s51228277.txt': {'end_ix': 75, 'start_ix': 75}},\n",
      "                       {'p15/p15078112/s58703686.txt': {'end_ix': 66, 'start_ix': 66}},\n",
      "                       {'p18/p18026902/s53920289.txt': {'end_ix': 63, 'start_ix': 63}},\n",
      "                       {'p18/p18026902/s51741672.txt': {'end_ix': 64, 'start_ix': 64}},\n",
      "                       {'p15/p15002538/s51219765.txt': {'end_ix': 46, 'start_ix': 46}},\n",
      "                       {'p18/p18019295/s57254021.txt': {'end_ix': 66, 'start_ix': 66}},\n",
      "                       {'p18/p18019295/s52492192.txt': {'end_ix': 44, 'start_ix': 44}},\n",
      "                       {'p10/p10002930/s55885481.txt': {'end_ix': 64, 'start_ix': 64}},\n",
      "                       {'p15/p15003988/s51924979.txt': {'end_ix': 69, 'start_ix': 69}},\n",
      "                       {'p15/p15002074/s52562523.txt': {'end_ix': 113, 'start_ix': 113}},\n",
      "                       {'p10/p10003637/s51461467.txt': {'end_ix': 74, 'start_ix': 74}},\n",
      "                       {'p15/p15000393/s59128455.txt': {'end_ix': 56, 'start_ix': 56}},\n",
      "                       {'p10/p10002428/s58581921.txt': {'end_ix': 33, 'start_ix': 33}},\n",
      "                       {'p15/p15003026/s53882677.txt': {'end_ix': 32, 'start_ix': 32}},\n",
      "                       {'p18/p18011616/s50630819.txt': {'end_ix': 54, 'start_ix': 54}},\n",
      "                       {'p18/p18028180/s53423151.txt': {'end_ix': 44, 'start_ix': 44}},\n",
      "                       {'p18/p18028180/s57601456.txt': {'end_ix': 35, 'start_ix': 35}},\n",
      "                       {'p18/p18000379/s59391463.txt': {'end_ix': 71, 'start_ix': 71}},\n",
      "                       {'p18/p18000379/s58920421.txt': {'end_ix': 81, 'start_ix': 81}}]}}\n",
      "{'clear': {'label': 'OBS-DP',\n",
      "           'normalization': {'UMLS': {'definition': {'definition': 'No results',\n",
      "                                                     'source': 'No results'},\n",
      "                                      'name': 'clear',\n",
      "                                      'semanticTypes': 'No results',\n",
      "                                      'ui': 'C2963144'}},\n",
      "           'reports': [{'p18/p18004941/s58821758.txt': {'end_ix': 38, 'start_ix': 38}},\n",
      "                       {'p18/p18004941/s58034164.txt': {'end_ix': 102, 'start_ix': 102}},\n",
      "                       {'p18/p18003081/s52532534.txt': {'end_ix': 59, 'start_ix': 59}},\n",
      "                       {'p18/p18003081/s50577366.txt': {'end_ix': 63, 'start_ix': 63}},\n",
      "                       {'p18/p18003081/s53302126.txt': {'end_ix': 59, 'start_ix': 59}},\n",
      "                       {'p18/p18023211/s50231046.txt': {'end_ix': 64, 'start_ix': 64}},\n",
      "                       {'p18/p18023211/s56555187.txt': {'end_ix': 90, 'start_ix': 90}},\n",
      "                       {'p18/p18001922/s52326539.txt': {'end_ix': 22, 'start_ix': 22}},\n",
      "                       {'p15/p15165563/s56337929.txt': {'end_ix': 115, 'start_ix': 115}},\n",
      "                       {'p15/p15165563/s57194811.txt': {'end_ix': 31, 'start_ix': 31}},\n",
      "                       {'p15/p15004061/s56845046.txt': {'end_ix': 47, 'start_ix': 47}},\n",
      "                       {'p15/p15001073/s52168754.txt': {'end_ix': 64, 'start_ix': 64}},\n",
      "                       {'p15/p15078112/s51228277.txt': {'end_ix': 78, 'start_ix': 78}},\n",
      "                       {'p15/p15078112/s59479753.txt': {'end_ix': 44, 'start_ix': 44}},\n",
      "                       {'p15/p15078112/s59479753.txt': {'end_ix': 85, 'start_ix': 85}},\n",
      "                       {'p15/p15078112/s58703686.txt': {'end_ix': 68, 'start_ix': 68}},\n",
      "                       {'p18/p18026902/s53920289.txt': {'end_ix': 64, 'start_ix': 64}},\n",
      "                       {'p18/p18026902/s51741672.txt': {'end_ix': 70, 'start_ix': 70}},\n",
      "                       {'p15/p15003294/s51224077.txt': {'end_ix': 36, 'start_ix': 36}},\n",
      "                       {'p15/p15003294/s51225513.txt': {'end_ix': 66, 'start_ix': 66}},\n",
      "                       {'p15/p15003294/s58755630.txt': {'end_ix': 24, 'start_ix': 24}},\n",
      "                       {'p15/p15003294/s54302152.txt': {'end_ix': 56, 'start_ix': 56}},\n",
      "                       {'p15/p15003294/s57007434.txt': {'end_ix': 29, 'start_ix': 29}},\n",
      "                       {'p18/p18012427/s54748444.txt': {'end_ix': 53, 'start_ix': 53}},\n",
      "                       {'p18/p18012427/s55382203.txt': {'end_ix': 71, 'start_ix': 71}},\n",
      "                       {'p18/p18012427/s54022564.txt': {'end_ix': 50, 'start_ix': 50}},\n",
      "                       {'p18/p18010079/s52730761.txt': {'end_ix': 45, 'start_ix': 45}},\n",
      "                       {'p15/p15002538/s58326480.txt': {'end_ix': 40, 'start_ix': 40}},\n",
      "                       {'p15/p15002538/s51219765.txt': {'end_ix': 48, 'start_ix': 48}},\n",
      "                       {'p18/p18019295/s57254021.txt': {'end_ix': 68, 'start_ix': 68}},\n",
      "                       {'p18/p18019295/s52794569.txt': {'end_ix': 38, 'start_ix': 38}},\n",
      "                       {'p18/p18002106/s56517301.txt': {'end_ix': 100, 'start_ix': 100}},\n",
      "                       {'p18/p18003894/s58464660.txt': {'end_ix': 60, 'start_ix': 60}},\n",
      "                       {'p18/p18003894/s56902065.txt': {'end_ix': 83, 'start_ix': 83}},\n",
      "                       {'p10/p10003502/s57812613.txt': {'end_ix': 57, 'start_ix': 57}},\n",
      "                       {'p15/p15006483/s57974872.txt': {'end_ix': 31, 'start_ix': 31}},\n",
      "                       {'p15/p15006483/s57012181.txt': {'end_ix': 80, 'start_ix': 80}},\n",
      "                       {'p10/p10002930/s55885481.txt': {'end_ix': 67, 'start_ix': 67}},\n",
      "                       {'p10/p10007795/s57491780.txt': {'end_ix': 90, 'start_ix': 90}},\n",
      "                       {'p10/p10007795/s56375093.txt': {'end_ix': 87, 'start_ix': 87}},\n",
      "                       {'p10/p10005866/s56175428.txt': {'end_ix': 135, 'start_ix': 135}},\n",
      "                       {'p15/p15002957/s50480029.txt': {'end_ix': 32, 'start_ix': 32}},\n",
      "                       {'p10/p10002661/s53368584.txt': {'end_ix': 36, 'start_ix': 36}},\n",
      "                       {'p15/p15003988/s51924979.txt': {'end_ix': 74, 'start_ix': 74}},\n",
      "                       {'p10/p10003052/s58630288.txt': {'end_ix': 48, 'start_ix': 48}},\n",
      "                       {'p18/p18017363/s53531162.txt': {'end_ix': 35, 'start_ix': 35}},\n",
      "                       {'p15/p15002074/s52562523.txt': {'end_ix': 116, 'start_ix': 116}},\n",
      "                       {'p10/p10005368/s54510564.txt': {'end_ix': 38, 'start_ix': 38}},\n",
      "                       {'p15/p15003122/s55694380.txt': {'end_ix': 61, 'start_ix': 61}},\n",
      "                       {'p18/p18015004/s57985812.txt': {'end_ix': 31, 'start_ix': 31}},\n",
      "                       {'p18/p18015004/s58110495.txt': {'end_ix': 46, 'start_ix': 46}},\n",
      "                       {'p18/p18015004/s56533254.txt': {'end_ix': 94, 'start_ix': 94}},\n",
      "                       {'p10/p10003637/s51461467.txt': {'end_ix': 76, 'start_ix': 76}},\n",
      "                       {'p18/p18002668/s52520031.txt': {'end_ix': 30, 'start_ix': 30}},\n",
      "                       {'p10/p10003019/s56635465.txt': {'end_ix': 79, 'start_ix': 79}},\n",
      "                       {'p10/p10003019/s56635465.txt': {'end_ix': 112, 'start_ix': 112}},\n",
      "                       {'p10/p10003019/s59005604.txt': {'end_ix': 64, 'start_ix': 64}},\n",
      "                       {'p10/p10003019/s55931751.txt': {'end_ix': 126, 'start_ix': 126}},\n",
      "                       {'p10/p10003019/s59607556.txt': {'end_ix': 121, 'start_ix': 121}},\n",
      "                       {'p18/p18001529/s58906888.txt': {'end_ix': 30, 'start_ix': 30}},\n",
      "                       {'p18/p18001649/s53184881.txt': {'end_ix': 45, 'start_ix': 45}},\n",
      "                       {'p18/p18001523/s58950691.txt': {'end_ix': 19, 'start_ix': 19}},\n",
      "                       {'p18/p18001523/s53676202.txt': {'end_ix': 53, 'start_ix': 53}},\n",
      "                       {'p15/p15003289/s54505463.txt': {'end_ix': 40, 'start_ix': 40}},\n",
      "                       {'p15/p15000393/s53701643.txt': {'end_ix': 87, 'start_ix': 87}},\n",
      "                       {'p15/p15000393/s51634677.txt': {'end_ix': 66, 'start_ix': 66}},\n",
      "                       {'p15/p15000393/s59128455.txt': {'end_ix': 59, 'start_ix': 59}},\n",
      "                       {'p10/p10002428/s57321224.txt': {'end_ix': 57, 'start_ix': 57}},\n",
      "                       {'p10/p10002428/s59258773.txt': {'end_ix': 49, 'start_ix': 49}},\n",
      "                       {'p10/p10002428/s56836542.txt': {'end_ix': 81, 'start_ix': 81}},\n",
      "                       {'p10/p10002428/s58581921.txt': {'end_ix': 97, 'start_ix': 97}},\n",
      "                       {'p10/p10002428/s57064083.txt': {'end_ix': 133, 'start_ix': 133}},\n",
      "                       {'p18/p18006842/s59597753.txt': {'end_ix': 84, 'start_ix': 84}},\n",
      "                       {'p15/p15002496/s51438936.txt': {'end_ix': 113, 'start_ix': 113}},\n",
      "                       {'p15/p15002496/s53899716.txt': {'end_ix': 37, 'start_ix': 37}},\n",
      "                       {'p15/p15002496/s55634030.txt': {'end_ix': 56, 'start_ix': 56}},\n",
      "                       {'p15/p15002496/s53190310.txt': {'end_ix': 74, 'start_ix': 74}},\n",
      "                       {'p15/p15002496/s57105647.txt': {'end_ix': 66, 'start_ix': 66}},\n",
      "                       {'p15/p15002496/s55374744.txt': {'end_ix': 48, 'start_ix': 48}},\n",
      "                       {'p18/p18001786/s56916492.txt': {'end_ix': 43, 'start_ix': 43}},\n",
      "                       {'p15/p15003026/s53882677.txt': {'end_ix': 34, 'start_ix': 34}},\n",
      "                       {'p15/p15002397/s57752049.txt': {'end_ix': 26, 'start_ix': 26}},\n",
      "                       {'p18/p18011616/s57021204.txt': {'end_ix': 74, 'start_ix': 74}},\n",
      "                       {'p18/p18011616/s50630819.txt': {'end_ix': 59, 'start_ix': 59}},\n",
      "                       {'p18/p18028180/s54705304.txt': {'end_ix': 37, 'start_ix': 37}},\n",
      "                       {'p18/p18028180/s50287111.txt': {'end_ix': 60, 'start_ix': 60}},\n",
      "                       {'p18/p18028180/s57601456.txt': {'end_ix': 41, 'start_ix': 41}},\n",
      "                       {'p18/p18000379/s59391463.txt': {'end_ix': 73, 'start_ix': 73}},\n",
      "                       {'p18/p18000379/s55947854.txt': {'end_ix': 63, 'start_ix': 63}},\n",
      "                       {'p18/p18000379/s58920421.txt': {'end_ix': 82, 'start_ix': 82}},\n",
      "                       {'p18/p18000379/s57979934.txt': {'end_ix': 73, 'start_ix': 73}},\n",
      "                       {'p15/p15002643/s51061808.txt': {'end_ix': 60, 'start_ix': 60}},\n",
      "                       {'p18/p18017572/s52030715.txt': {'end_ix': 38, 'start_ix': 38}},\n",
      "                       {'p18/p18017572/s54836720.txt': {'end_ix': 73, 'start_ix': 73}},\n",
      "                       {'p15/p15001834/s54486241.txt': {'end_ix': 64, 'start_ix': 64}},\n",
      "                       {'p15/p15001834/s54265476.txt': {'end_ix': 28, 'start_ix': 28}},\n",
      "                       {'p15/p15001834/s52944718.txt': {'end_ix': 24, 'start_ix': 24}},\n",
      "                       {'p15/p15001834/s50931830.txt': {'end_ix': 26, 'start_ix': 26}}]}}\n",
      "{'Normal': {'label': 'OBS-DP',\n",
      "            'normalization': {'UMLS': {'definition': {'definition': 'In pathology, a term that is '\n",
      "                                                                    'used to describe a tissue '\n",
      "                                                                    'specimen that has a normal '\n",
      "                                                                    'appearance.',\n",
      "                                                      'source': 'NCI'},\n",
      "                                       'name': 'Normal',\n",
      "                                       'semanticTypes': 'Qualitative Concept',\n",
      "                                       'ui': 'C0205307'}},\n",
      "            'reports': [{'p18/p18004941/s58821758.txt': {'end_ix': 40, 'start_ix': 40}},\n",
      "                        {'p10/p10004457/s55439624.txt': {'end_ix': 62, 'start_ix': 62}},\n",
      "                        {'p10/p10004457/s55439624.txt': {'end_ix': 72, 'start_ix': 72}},\n",
      "                        {'p18/p18026902/s58219787.txt': {'end_ix': 89, 'start_ix': 89}},\n",
      "                        {'p15/p15002035/s52843277.txt': {'end_ix': 46, 'start_ix': 46}},\n",
      "                        {'p15/p15003988/s51924979.txt': {'end_ix': 111, 'start_ix': 111}},\n",
      "                        {'p10/p10005368/s57426090.txt': {'end_ix': 70, 'start_ix': 70}},\n",
      "                        {'p15/p15003122/s55694380.txt': {'end_ix': 52, 'start_ix': 52}},\n",
      "                        {'p10/p10003019/s59730608.txt': {'end_ix': 56, 'start_ix': 56}},\n",
      "                        {'p18/p18001529/s58906888.txt': {'end_ix': 52, 'start_ix': 52}},\n",
      "                        {'p15/p15003289/s54505463.txt': {'end_ix': 79, 'start_ix': 79}},\n",
      "                        {'p10/p10002559/s52212843.txt': {'end_ix': 43, 'start_ix': 43}},\n",
      "                        {'p10/p10002559/s52212843.txt': {'end_ix': 68, 'start_ix': 68}},\n",
      "                        {'p10/p10002559/s52212843.txt': {'end_ix': 75, 'start_ix': 75}}]}}\n",
      "{'cardiomediastinal': {'label': 'ANAT-DP',\n",
      "                       'normalization': None,\n",
      "                       'reports': [{'p18/p18004941/s58821758.txt': {'end_ix': 41, 'start_ix': 41}},\n",
      "                                   {'p18/p18004941/s55576421.txt': {'end_ix': 37, 'start_ix': 37}},\n",
      "                                   {'p18/p18003081/s57007000.txt': {'end_ix': 66, 'start_ix': 66}},\n",
      "                                   {'p18/p18003081/s51438902.txt': {'end_ix': 109,\n",
      "                                                                    'start_ix': 109}},\n",
      "                                   {'p15/p15024484/s59036766.txt': {'end_ix': 75, 'start_ix': 75}},\n",
      "                                   {'p18/p18001922/s50114627.txt': {'end_ix': 63, 'start_ix': 63}},\n",
      "                                   {'p15/p15165563/s54509771.txt': {'end_ix': 129,\n",
      "                                                                    'start_ix': 129}},\n",
      "                                   {'p15/p15165563/s53383993.txt': {'end_ix': 72, 'start_ix': 72}},\n",
      "                                   {'p15/p15002183/s52341485.txt': {'end_ix': 32, 'start_ix': 32}},\n",
      "                                   {'p15/p15078112/s53834915.txt': {'end_ix': 88, 'start_ix': 88}},\n",
      "                                   {'p15/p15078112/s56357121.txt': {'end_ix': 76, 'start_ix': 76}},\n",
      "                                   {'p18/p18026902/s58219787.txt': {'end_ix': 91, 'start_ix': 91}},\n",
      "                                   {'p15/p15003294/s51224077.txt': {'end_ix': 39, 'start_ix': 39}},\n",
      "                                   {'p15/p15003294/s51225513.txt': {'end_ix': 50, 'start_ix': 50}},\n",
      "                                   {'p15/p15003296/s58126961.txt': {'end_ix': 57, 'start_ix': 57}},\n",
      "                                   {'p15/p15007487/s54185768.txt': {'end_ix': 74, 'start_ix': 74}},\n",
      "                                   {'p15/p15007487/s59034151.txt': {'end_ix': 99, 'start_ix': 99}},\n",
      "                                   {'p15/p15007487/s59752695.txt': {'end_ix': 74, 'start_ix': 74}},\n",
      "                                   {'p15/p15007487/s55512501.txt': {'end_ix': 81, 'start_ix': 81}},\n",
      "                                   {'p18/p18000735/s50785186.txt': {'end_ix': 36, 'start_ix': 36}},\n",
      "                                   {'p18/p18019295/s52492192.txt': {'end_ix': 38, 'start_ix': 38}},\n",
      "                                   {'p18/p18019295/s52794569.txt': {'end_ix': 28, 'start_ix': 28}},\n",
      "                                   {'p18/p18003894/s58464660.txt': {'end_ix': 98, 'start_ix': 98}},\n",
      "                                   {'p18/p18003894/s54842645.txt': {'end_ix': 72, 'start_ix': 72}},\n",
      "                                   {'p15/p15002957/s50480029.txt': {'end_ix': 35, 'start_ix': 35}},\n",
      "                                   {'p15/p15003122/s55694380.txt': {'end_ix': 53, 'start_ix': 53}},\n",
      "                                   {'p10/p10004235/s52962553.txt': {'end_ix': 65, 'start_ix': 65}},\n",
      "                                   {'p10/p10004235/s54234360.txt': {'end_ix': 39, 'start_ix': 39}},\n",
      "                                   {'p10/p10004235/s58604118.txt': {'end_ix': 104,\n",
      "                                                                    'start_ix': 104}},\n",
      "                                   {'p10/p10003019/s53934356.txt': {'end_ix': 99, 'start_ix': 99}},\n",
      "                                   {'p15/p15001393/s54726934.txt': {'end_ix': 43, 'start_ix': 43}},\n",
      "                                   {'p18/p18001523/s58950691.txt': {'end_ix': 31, 'start_ix': 31}},\n",
      "                                   {'p18/p18001523/s59117874.txt': {'end_ix': 30, 'start_ix': 30}},\n",
      "                                   {'p18/p18001523/s53676202.txt': {'end_ix': 56, 'start_ix': 56}},\n",
      "                                   {'p15/p15003289/s54505463.txt': {'end_ix': 57, 'start_ix': 57}},\n",
      "                                   {'p15/p15000393/s52929930.txt': {'end_ix': 69, 'start_ix': 69}},\n",
      "                                   {'p15/p15000393/s59128455.txt': {'end_ix': 61, 'start_ix': 61}},\n",
      "                                   {'p15/p15000393/s55479181.txt': {'end_ix': 30, 'start_ix': 30}},\n",
      "                                   {'p10/p10002428/s56836542.txt': {'end_ix': 86, 'start_ix': 86}},\n",
      "                                   {'p10/p10002428/s59659695.txt': {'end_ix': 41, 'start_ix': 41}},\n",
      "                                   {'p10/p10002428/s58658824.txt': {'end_ix': 59, 'start_ix': 59}},\n",
      "                                   {'p15/p15002496/s55374744.txt': {'end_ix': 70, 'start_ix': 70}},\n",
      "                                   {'p18/p18001786/s56916492.txt': {'end_ix': 46, 'start_ix': 46}},\n",
      "                                   {'p18/p18011616/s54058536.txt': {'end_ix': 50, 'start_ix': 50}},\n",
      "                                   {'p18/p18011616/s56212412.txt': {'end_ix': 49, 'start_ix': 49}},\n",
      "                                   {'p18/p18011616/s59704184.txt': {'end_ix': 55, 'start_ix': 55}},\n",
      "                                   {'p18/p18011616/s50630819.txt': {'end_ix': 45, 'start_ix': 45}},\n",
      "                                   {'p18/p18011616/s50977977.txt': {'end_ix': 50, 'start_ix': 50}},\n",
      "                                   {'p18/p18028180/s54705304.txt': {'end_ix': 42, 'start_ix': 42}},\n",
      "                                   {'p18/p18028180/s50287111.txt': {'end_ix': 48, 'start_ix': 48}},\n",
      "                                   {'p18/p18028180/s55973443.txt': {'end_ix': 60, 'start_ix': 60}},\n",
      "                                   {'p18/p18017572/s52030715.txt': {'end_ix': 73, 'start_ix': 73}},\n",
      "                                   {'p15/p15001834/s54486241.txt': {'end_ix': 74, 'start_ix': 74}},\n",
      "                                   {'p15/p15001834/s56269673.txt': {'end_ix': 46,\n",
      "                                                                    'start_ix': 46}}]}}\n",
      "{'hilar': {'label': 'ANAT-DP',\n",
      "           'normalization': {'UMLS': {'definition': {'definition': 'Refers to the area associated '\n",
      "                                                                   'with the hilum.',\n",
      "                                                     'source': 'NCI'},\n",
      "                                      'name': 'hilar',\n",
      "                                      'semanticTypes': 'Spatial Concept',\n",
      "                                      'ui': 'C0205150'}},\n",
      "           'reports': [{'p18/p18004941/s58821758.txt': {'end_ix': 43, 'start_ix': 43}},\n",
      "                       {'p18/p18004941/s58034164.txt': {'end_ix': 78, 'start_ix': 78}},\n",
      "                       {'p18/p18004941/s52289350.txt': {'end_ix': 40, 'start_ix': 40}},\n",
      "                       {'p18/p18004941/s55576421.txt': {'end_ix': 39, 'start_ix': 39}},\n",
      "                       {'p18/p18003081/s51239857.txt': {'end_ix': 47, 'start_ix': 47}},\n",
      "                       {'p15/p15165563/s56337929.txt': {'end_ix': 127, 'start_ix': 127}},\n",
      "                       {'p15/p15165563/s59506127.txt': {'end_ix': 84, 'start_ix': 84}},\n",
      "                       {'p15/p15004061/s56845046.txt': {'end_ix': 51, 'start_ix': 51}},\n",
      "                       {'p15/p15001073/s52168754.txt': {'end_ix': 33, 'start_ix': 33}},\n",
      "                       {'p15/p15078112/s58703686.txt': {'end_ix': 55, 'start_ix': 55}},\n",
      "                       {'p15/p15003294/s58755630.txt': {'end_ix': 42, 'start_ix': 42}},\n",
      "                       {'p18/p18012427/s54748444.txt': {'end_ix': 28, 'start_ix': 28}},\n",
      "                       {'p18/p18012427/s55382203.txt': {'end_ix': 90, 'start_ix': 90}},\n",
      "                       {'p15/p15003296/s54535760.txt': {'end_ix': 66, 'start_ix': 66}},\n",
      "                       {'p18/p18022445/s56006486.txt': {'end_ix': 160, 'start_ix': 160}},\n",
      "                       {'p15/p15007487/s54185768.txt': {'end_ix': 77, 'start_ix': 77}},\n",
      "                       {'p15/p15007487/s59034151.txt': {'end_ix': 102, 'start_ix': 102}},\n",
      "                       {'p15/p15007487/s55512501.txt': {'end_ix': 84, 'start_ix': 84}},\n",
      "                       {'p15/p15001501/s55616331.txt': {'end_ix': 63, 'start_ix': 63}},\n",
      "                       {'p18/p18019295/s57254021.txt': {'end_ix': 55, 'start_ix': 55}},\n",
      "                       {'p18/p18019295/s52794569.txt': {'end_ix': 30, 'start_ix': 30}},\n",
      "                       {'p18/p18002106/s56517301.txt': {'end_ix': 86, 'start_ix': 86}},\n",
      "                       {'p18/p18001942/s56433237.txt': {'end_ix': 56, 'start_ix': 56}},\n",
      "                       {'p15/p15002678/s50618143.txt': {'end_ix': 102, 'start_ix': 102}},\n",
      "                       {'p18/p18028999/s56651721.txt': {'end_ix': 36, 'start_ix': 36}},\n",
      "                       {'p18/p18020943/s56320518.txt': {'end_ix': 120, 'start_ix': 120}},\n",
      "                       {'p18/p18020943/s57074556.txt': {'end_ix': 49, 'start_ix': 49}},\n",
      "                       {'p18/p18022983/s51605945.txt': {'end_ix': 150, 'start_ix': 150}},\n",
      "                       {'p18/p18022983/s57985067.txt': {'end_ix': 87, 'start_ix': 87}},\n",
      "                       {'p18/p18022983/s57381264.txt': {'end_ix': 84, 'start_ix': 84}},\n",
      "                       {'p10/p10007795/s54492585.txt': {'end_ix': 34, 'start_ix': 34}},\n",
      "                       {'p10/p10007795/s56375093.txt': {'end_ix': 61, 'start_ix': 61}},\n",
      "                       {'p10/p10005866/s56175428.txt': {'end_ix': 90, 'start_ix': 90}},\n",
      "                       {'p15/p15003988/s51924979.txt': {'end_ix': 94, 'start_ix': 94}},\n",
      "                       {'p15/p15003122/s55694380.txt': {'end_ix': 55, 'start_ix': 55}},\n",
      "                       {'p10/p10004235/s59895508.txt': {'end_ix': 64, 'start_ix': 64}},\n",
      "                       {'p10/p10003637/s51461467.txt': {'end_ix': 69, 'start_ix': 69}},\n",
      "                       {'p10/p10003019/s52117631.txt': {'end_ix': 132, 'start_ix': 132}},\n",
      "                       {'p10/p10003019/s53934356.txt': {'end_ix': 102, 'start_ix': 102}},\n",
      "                       {'p18/p18001529/s58906888.txt': {'end_ix': 34, 'start_ix': 34}},\n",
      "                       {'p10/p10004322/s57662923.txt': {'end_ix': 150, 'start_ix': 150}},\n",
      "                       {'p18/p18001523/s53676202.txt': {'end_ix': 58, 'start_ix': 58}},\n",
      "                       {'p15/p15000393/s59788459.txt': {'end_ix': 119, 'start_ix': 119}},\n",
      "                       {'p15/p15000393/s57099953.txt': {'end_ix': 106, 'start_ix': 106}},\n",
      "                       {'p15/p15000393/s55479181.txt': {'end_ix': 32, 'start_ix': 32}},\n",
      "                       {'p10/p10002428/s57321224.txt': {'end_ix': 34, 'start_ix': 34}},\n",
      "                       {'p10/p10002428/s56836542.txt': {'end_ix': 84, 'start_ix': 84}},\n",
      "                       {'p10/p10002428/s57506266.txt': {'end_ix': 87, 'start_ix': 87}},\n",
      "                       {'p10/p10002428/s58581921.txt': {'end_ix': 158, 'start_ix': 158}},\n",
      "                       {'p18/p18006842/s55078395.txt': {'end_ix': 63, 'start_ix': 63}},\n",
      "                       {'p15/p15002496/s52862073.txt': {'end_ix': 53, 'start_ix': 53}},\n",
      "                       {'p15/p15002496/s55634030.txt': {'end_ix': 68, 'start_ix': 68}},\n",
      "                       {'p15/p15002496/s57105647.txt': {'end_ix': 37, 'start_ix': 37}},\n",
      "                       {'p15/p15001474/s51737379.txt': {'end_ix': 69, 'start_ix': 69}},\n",
      "                       {'p18/p18001786/s56916492.txt': {'end_ix': 49, 'start_ix': 49}},\n",
      "                       {'p15/p15007710/s53803512.txt': {'end_ix': 31, 'start_ix': 31}},\n",
      "                       {'p18/p18011616/s57021204.txt': {'end_ix': 50, 'start_ix': 50}},\n",
      "                       {'p18/p18011616/s50630819.txt': {'end_ix': 47, 'start_ix': 47}},\n",
      "                       {'p18/p18028180/s57844688.txt': {'end_ix': 113, 'start_ix': 113}},\n",
      "                       {'p18/p18028180/s50287111.txt': {'end_ix': 50, 'start_ix': 50}},\n",
      "                       {'p18/p18000379/s59391463.txt': {'end_ix': 59, 'start_ix': 59}},\n",
      "                       {'p18/p18000379/s55947854.txt': {'end_ix': 50, 'start_ix': 50}},\n",
      "                       {'p15/p15002643/s51061808.txt': {'end_ix': 52, 'start_ix': 52}},\n",
      "                       {'p10/p10002559/s52212843.txt': {'end_ix': 76, 'start_ix': 76}},\n",
      "                       {'p15/p15001834/s50931830.txt': {'end_ix': 30, 'start_ix': 30}}]}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Specify the file path\n",
    "file_path = '../resource/all_unique_entities_normalized.json'\n",
    "\n",
    "# Read the JSON file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    all_entities = json.load(f)\n",
    "\n",
    "# print the number of entities\n",
    "print(f\"Number of entities: {len(all_entities)}\")\n",
    "\n",
    "# pretty print the fist 5 entities\n",
    "print(\"First 5 items:\")\n",
    "for i, (key, value) in enumerate(all_entities.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    pprint({key:value}, width=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总entity数量: 1250\n",
      "\n",
      "每个label的entity数量:\n",
      "ANAT-DP: 363\n",
      "OBS-DP: 712\n",
      "OBS-DA: 93\n",
      "OBS-U: 82\n",
      "\n",
      "出现次数最多的entity: 'pleural' (出现在 424 个报告中)\n",
      "\n",
      "有normalization的entity数量: 0\n",
      "没有normalization的entity数量: 1250\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 统计每个label的entity数量\n",
    "label_counts = Counter()\n",
    "for entity, data in all_entities.items():\n",
    "    label_counts[data['label']] += 1\n",
    "\n",
    "# 统计总的entity数量\n",
    "total_entities = len(all_entities)\n",
    "\n",
    "# 找出出现次数最多的entity\n",
    "most_common_entity = max(all_entities.items(), key=lambda x: len(x[1]['reports']))\n",
    "\n",
    "# 计算平均报告数量\n",
    "avg_reports = sum(len(data['reports']) for data in all_entities.values()) / total_entities\n",
    "\n",
    "print(f\"总entity数量: {total_entities}\")\n",
    "print(\"\\n每个label的entity数量:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "print(f\"\\n出现次数最多的entity: '{most_common_entity[0]}' (出现在 {len(most_common_entity[1]['reports'])} 个报告中)\")\n",
    "\n",
    "# 统计有normalization和没有normalization的entity数量\n",
    "with_norm = sum(1 for data in all_entities.values() if data['normalization'] is not None)\n",
    "without_norm = total_entities - with_norm\n",
    "\n",
    "print(f\"\\n有normalization的entity数量: {with_norm}\")\n",
    "print(f\"没有normalization的entity数量: {without_norm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file all_unique_entities.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 1250\n",
      "总entity数量: 1250\n",
      "\n",
      "每个label的entity数量:\n",
      "ANAT-DP: 363\n",
      "OBS-DP: 712\n",
      "OBS-DA: 93\n",
      "OBS-U: 82\n",
      "\n",
      "有normalization的entity数量: 498\n",
      "没有normalization的entity数量: 752\n",
      "\n",
      "有normalization的entity占比: 39.84%\n",
      "\n",
      "每个label的有normalization的entity数量:\n",
      "ANAT-DP: 152\n",
      "ANAT-DP的有normalization的entity占比: 41.87%\n",
      "OBS-DP: 268\n",
      "OBS-DP的有normalization的entity占比: 37.64%\n",
      "OBS-DA: 41\n",
      "OBS-DA的有normalization的entity占比: 44.09%\n",
      "OBS-U: 37\n",
      "OBS-U的有normalization的entity占比: 45.12%\n",
      "\n",
      "每个label的没有normalization的entity数量:\n",
      "ANAT-DP: 211\n",
      "OBS-DP: 444\n",
      "OBS-DA: 52\n",
      "OBS-U: 45\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Specify the file path\n",
    "file_path = '../resource/all_unique_entities_normalized.json'\n",
    "\n",
    "# Read the JSON file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    all_entities = json.load(f)\n",
    "\n",
    "# print the number of entities\n",
    "print(f\"Number of entities: {len(all_entities)}\")\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# 统计每个label的entity数量\n",
    "label_counts = Counter()\n",
    "for entity, data in all_entities.items():\n",
    "    label_counts[data['label']] += 1\n",
    "\n",
    "# 统计总的entity数量\n",
    "total_entities = len(all_entities)\n",
    "\n",
    "# 找出出现次数最多的entity\n",
    "most_common_entity = max(all_entities.items(), key=lambda x: len(x[1]['reports']))\n",
    "\n",
    "# 计算平均报告数量\n",
    "avg_reports = sum(len(data['reports']) for data in all_entities.values()) / total_entities\n",
    "\n",
    "print(\"\\n每个label的entity数量:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "\n",
    "# 统计有normalization和没有normalization的entity数量\n",
    "with_norm = sum(1 for data in all_entities.values() if data['normalization'] is not None)\n",
    "without_norm = total_entities - with_norm\n",
    "\n",
    "print(f\"\\n有normalization的entity数量: {with_norm}\")\n",
    "print(f\"没有normalization的entity数量: {without_norm}\")\n",
    "# percentage of entities with normalization\n",
    "print(f\"\\n有normalization的entity占比: {with_norm / total_entities:.2%}\")\n",
    "\n",
    "\n",
    "# 统计每个label的有normalization和没有normalization的entity数量\n",
    "label_counts_with_norm = Counter()\n",
    "label_counts_without_norm = Counter()\n",
    "for entity, data in all_entities.items():\n",
    "    if data['normalization'] is not None:\n",
    "        label_counts_with_norm[data['label']] += 1\n",
    "    else:\n",
    "        label_counts_without_norm[data['label']] += 1\n",
    "\n",
    "print(\"\\n每个label的有normalization的entity数量:\")\n",
    "for label, count in label_counts_with_norm.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "    # percentage of entities with normalization\n",
    "    print(f\"{label}的有normalization的entity占比: {count / label_counts[label]:.2%}\")\n",
    "\n",
    "\n",
    "print(\"\\n每个label的没有normalization的entity数量:\")\n",
    "for label, count in label_counts_without_norm.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First normalization\n",
    "result in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entities: 1250\n",
      "Number of items with normalization: 498\n",
      "Number of items with normalization: null: 752\n",
      "\n",
      "Number of items with normalization.umls.ui == 'No results': 120\n",
      "This means there is even no candidate in UMLS for this entity\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "file_path = '../resource/all_unique_entities_normalized.json'\n",
    "\n",
    "# Read the JSON file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    all_entities = json.load(f)\n",
    "\n",
    "# Print total number of entities\n",
    "print(f\"Total number of entities: {len(all_entities)}\")\n",
    "\n",
    "\n",
    "# Count the number of items with normalization\n",
    "count_with_normalization = sum(1 for data in all_entities.values() if data['normalization'] is not None)\n",
    "print(f\"Number of items with normalization: {count_with_normalization}\")\n",
    "\n",
    "# Count the number of items with normalization: null\n",
    "count_null_normalization = sum(1 for data in all_entities.values() if data['normalization'] is None)\n",
    "print(f\"Number of items with normalization: null: {count_null_normalization}\")\n",
    "\n",
    "\n",
    "count_no_results = sum(1 for data in all_entities.values() if data['normalization'] and data['normalization']['UMLS'] and data['normalization']['UMLS']['definition']['definition'] == \"No results\")\n",
    "print(f\"\\nNumber of items with normalization.umls.ui == 'No results': {count_no_results}\")\n",
    "print('This means there is even no candidate in UMLS for this entity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Types Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different unique kinds of items with normalization.umls.semanticTypes: 46\n",
      "Unique kinds of items with normalization.umls.semanticTypes:\n",
      "{'Immunologic Factor', 'Acquired Abnormality', 'Activity', 'Medical Device', 'Intellectual Product', 'Event', 'Pharmacologic Substance', 'Injury or Poisoning', 'Finding', 'Mental or Behavioral Dysfunction', 'Functional Concept', 'Idea or Concept', 'Organic Chemical', 'Substance', 'Diagnostic Procedure', 'Anatomical Abnormality', 'Natural Phenomenon or Process', 'Patient or Disabled Group', 'Organism Function', 'Body Substance', 'Research Activity', 'Phenomenon or Process', 'Chemical Viewed Functionally', 'Pathologic Function', 'Clinical Attribute', 'Molecular Function', 'Congenital Abnormality', 'Anatomical Structure', 'Hazardous or Poisonous Substance', 'Disease or Syndrome', 'Quantitative Concept', 'Conceptual Entity', 'Body Part, Organ, or Organ Component', 'Laboratory or Test Result', 'Qualitative Concept', 'Chemical Viewed Structurally', 'Tissue', 'Body System', 'Manufactured Object', 'Spatial Concept', 'Body Location or Region', 'Body Space or Junction', 'Temporal Concept', 'Cell Component', 'Therapeutic or Preventive Procedure', 'Social Behavior'}\n",
      "\n",
      "Semantic type counts:\n",
      "1. Spatial Concept: 118\n",
      "2. Qualitative Concept: 95\n",
      "3. Body Part, Organ, or Organ Component: 35\n",
      "4. Finding: 34\n",
      "5. Quantitative Concept: 33\n",
      "6. Functional Concept: 29\n",
      "7. Pathologic Function: 18\n",
      "8. Temporal Concept: 17\n",
      "9. Disease or Syndrome: 14\n",
      "10. Body Location or Region: 14\n",
      "11. Medical Device: 10\n",
      "12. Idea or Concept: 9\n",
      "13. Therapeutic or Preventive Procedure: 7\n",
      "14. Conceptual Entity: 6\n",
      "15. Tissue: 4\n",
      "16. Manufactured Object: 4\n",
      "17. Activity: 3\n",
      "18. Intellectual Product: 3\n",
      "19. Pharmacologic Substance: 3\n",
      "20. Injury or Poisoning: 3\n",
      "21. Anatomical Abnormality: 3\n",
      "22. Chemical Viewed Functionally: 3\n",
      "23. Anatomical Structure: 3\n",
      "24. Acquired Abnormality: 2\n",
      "25. Organic Chemical: 2\n",
      "26. Natural Phenomenon or Process: 2\n",
      "27. Congenital Abnormality: 2\n",
      "28. Body Space or Junction: 2\n",
      "29. Cell Component: 2\n",
      "30. Social Behavior: 2\n",
      "31. Immunologic Factor: 1\n",
      "32. Event: 1\n",
      "33. Mental or Behavioral Dysfunction: 1\n",
      "34. Substance: 1\n",
      "35. Diagnostic Procedure: 1\n",
      "36. Patient or Disabled Group: 1\n",
      "37. Organism Function: 1\n",
      "38. Body Substance: 1\n",
      "39. Research Activity: 1\n",
      "40. Phenomenon or Process: 1\n",
      "41. Clinical Attribute: 1\n",
      "42. Molecular Function: 1\n",
      "43. Hazardous or Poisonous Substance: 1\n",
      "44. Laboratory or Test Result: 1\n",
      "45. Chemical Viewed Structurally: 1\n",
      "46. Body System: 1\n",
      "\n",
      "Total counts: 498\n"
     ]
    }
   ],
   "source": [
    "semantic_types = set()\n",
    "\n",
    "for entity, data in all_entities.items():\n",
    "    if data.get('normalization') and data['normalization'].get('UMLS'):\n",
    "        semantic_types.add(data['normalization']['UMLS']['semanticTypes'])\n",
    "\n",
    "num_semantic_types = len(semantic_types)\n",
    "\n",
    "print(f\"Number of different unique kinds of items with normalization.umls.semanticTypes: {num_semantic_types}\")\n",
    "\n",
    "print(\"Unique kinds of items with normalization.umls.semanticTypes:\")\n",
    "print(semantic_types)\n",
    "\n",
    "\n",
    "# Count the number of each unique kind of item\n",
    "semantic_type_counts = {}\n",
    "for semantic_type in semantic_types:\n",
    "    semantic_type_counts[semantic_type] = sum(1 for entity, data in all_entities.items() if data.get('normalization') and data['normalization'].get('UMLS') and data['normalization']['UMLS']['semanticTypes'] == semantic_type)\n",
    "\n",
    "# 降序输出 semantic_type_counts，带序号\n",
    "print(\"\\nSemantic type counts:\")\n",
    "for i, (semantic_type, count) in enumerate(sorted(semantic_type_counts.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "    print(f\"{i}. {semantic_type}: {count}\")\n",
    "    \n",
    "\n",
    "\n",
    "#  sum of the counts\n",
    "total_counts = sum(semantic_type_counts.values())\n",
    "print(f\"\\nTotal counts: {total_counts}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity with no normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entities: 1250\n",
      "Number of entities with normalization: null: 752\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "file_path = '../resource/all_unique_entities_normalized.json'\n",
    "\n",
    "# Read the JSON file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    all_entities = json.load(f)\n",
    "\n",
    "# Print total number of entities\n",
    "print(f\"Total number of entities: {len(all_entities)}\")\n",
    "\n",
    "# get list of all entities with normalization: null\n",
    "entities_null_normalization = [entity for entity, data in all_entities.items() if data['normalization'] is None]\n",
    "# print(f\"Entities with normalization: null: {entities_null_normalization}\")\n",
    "\n",
    "# len(entities_null_normalization)\n",
    "print(f\"Number of entities with normalization: null: {len(entities_null_normalization)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cardiomediastinal',\n",
      " 'silhouettes',\n",
      " 'surfaces',\n",
      " 'approximately a 4.6 cm',\n",
      " 'carina',\n",
      " 'distal',\n",
      " 'advanced by at least 11 cm',\n",
      " 'contours',\n",
      " 'grossly',\n",
      " 'unremarkable',\n",
      " 'Pulmonary',\n",
      " 'apex',\n",
      " 'slightly',\n",
      " \"patient 's chin\",\n",
      " 'displaced',\n",
      " 'osseous',\n",
      " 'Standard',\n",
      " 'positioning',\n",
      " 'mid',\n",
      " 'portion',\n",
      " 'body',\n",
      " 'The side - port',\n",
      " 'junction',\n",
      " 'Otherwise',\n",
      " 'change',\n",
      " 'stable',\n",
      " 'ET',\n",
      " '4.3 cm',\n",
      " 'NG',\n",
      " 'malpositioned',\n",
      " 'position',\n",
      " 'enteric',\n",
      " 'approximately 1.8 cm',\n",
      " 'barely',\n",
      " 'side ports',\n",
      " 'approximately 12 cm',\n",
      " 'approximately 2 cm',\n",
      " 'overt',\n",
      " 'CHF',\n",
      " 'silhouette',\n",
      " 'Limited',\n",
      " 'This',\n",
      " 'cardiac',\n",
      " 'mildly',\n",
      " 'unchanged',\n",
      " 'bronchovascular',\n",
      " 'pulmonary',\n",
      " 'patchy',\n",
      " 'bibasilar',\n",
      " 'opacities',\n",
      " 'abnormalities',\n",
      " 'Patchy',\n",
      " 'parenchymal',\n",
      " 'opacity',\n",
      " 'frontal',\n",
      " 'radiograph',\n",
      " 'scoliosis',\n",
      " 'asymmetry',\n",
      " 'ribcage',\n",
      " 'Mildly',\n",
      " 'underinflated',\n",
      " 'largely',\n",
      " 'Osseous',\n",
      " 'The lateral view is limited',\n",
      " \"patient 's arms\",\n",
      " 'lower',\n",
      " 'They',\n",
      " 'conspicuous',\n",
      " 'recent exam from ___',\n",
      " 'similar',\n",
      " 'previous exams from ___',\n",
      " 'rib',\n",
      " 'aspiration',\n",
      " 'difficult to assess',\n",
      " 'potentially',\n",
      " 'obscuring',\n",
      " 'cannot be assessed',\n",
      " 'findings',\n",
      " 'Evaluation is limited',\n",
      " \"patient 's head\",\n",
      " 'apices',\n",
      " 'visualized',\n",
      " 'increasing',\n",
      " 'substantial',\n",
      " 'layering',\n",
      " 'compressive',\n",
      " 'loculation',\n",
      " 'Cardiomediastinal',\n",
      " 'border',\n",
      " 'right - sided',\n",
      " 'subjacent',\n",
      " 'well',\n",
      " 'aerated',\n",
      " 'TIPS',\n",
      " 'round',\n",
      " 'relevant',\n",
      " 'Unchanged',\n",
      " 'Superimposed',\n",
      " 'limits',\n",
      " 'unfolded',\n",
      " 'partially',\n",
      " 'Bibasilar',\n",
      " 'airspace',\n",
      " 'costophrenic',\n",
      " 'overlying',\n",
      " 'wires',\n",
      " 'laterally',\n",
      " 'not well assessed',\n",
      " 'monitoring',\n",
      " 'support',\n",
      " 'mild',\n",
      " 'post - operative',\n",
      " 'sternal',\n",
      " 'Mild',\n",
      " 'retrocardiac',\n",
      " 'Stable',\n",
      " 'artery',\n",
      " 'enlargement',\n",
      " 'subclavian',\n",
      " 'PICC',\n",
      " 'SVC',\n",
      " 'pressure',\n",
      " 'sharply',\n",
      " 'hemidiaphragm',\n",
      " 'an',\n",
      " 'Cardiac',\n",
      " 'top - normal',\n",
      " 'hypertension',\n",
      " 'prominence',\n",
      " 'atrium',\n",
      " 'valve',\n",
      " 'origin',\n",
      " 'infectious',\n",
      " 'engorgement',\n",
      " 'layer',\n",
      " 'dependently',\n",
      " 'peculiar',\n",
      " 'pericardial',\n",
      " 'relatively',\n",
      " 'contour',\n",
      " 'changed',\n",
      " 'hilus',\n",
      " 'not less distinct',\n",
      " 'overload',\n",
      " 'out of review',\n",
      " 'Multiple',\n",
      " 'multiple',\n",
      " 'deformities',\n",
      " 'thoracic',\n",
      " 'ribs',\n",
      " 'fifith',\n",
      " 'Slight',\n",
      " 'posterior',\n",
      " 'calcified',\n",
      " 'Severe',\n",
      " 'standard',\n",
      " 'below',\n",
      " 'diaphragm',\n",
      " 'margin',\n",
      " 'thickening',\n",
      " 'S type',\n",
      " 'configuration',\n",
      " 'similar to prior',\n",
      " 'scarring',\n",
      " 'streaky',\n",
      " 'superimposed',\n",
      " 'top',\n",
      " 'wall',\n",
      " 'surgery',\n",
      " 'slight',\n",
      " 'hyperinflated',\n",
      " 'Sternotomy',\n",
      " 'median',\n",
      " 'sternotomy',\n",
      " 'CABG',\n",
      " 'obstructive',\n",
      " 'Degenerative',\n",
      " 'changes',\n",
      " 'along',\n",
      " 'spine',\n",
      " 'COPD',\n",
      " 'blunting',\n",
      " 'trace',\n",
      " 'knob',\n",
      " 'calcifications',\n",
      " 'multilevel',\n",
      " 'degenerative',\n",
      " 'airway',\n",
      " 'post CABG',\n",
      " 'appropriately',\n",
      " 'aligned',\n",
      " 'diaphragms',\n",
      " 'markings',\n",
      " 'institial',\n",
      " 'infiltrate',\n",
      " 'frank',\n",
      " 'subtle',\n",
      " '9 mm',\n",
      " 'otherwise',\n",
      " 'aspect',\n",
      " 'elongation',\n",
      " 'flattening',\n",
      " 'hemidiaphragms',\n",
      " 'resolving',\n",
      " 'hila',\n",
      " 'Improving',\n",
      " 'moderately',\n",
      " 'AP',\n",
      " 'diameter',\n",
      " 'Lower',\n",
      " 'secondary',\n",
      " 'accentuation',\n",
      " 'Ovoid',\n",
      " 'bilaterally',\n",
      " 'infiltrates',\n",
      " 'free',\n",
      " 'Supportive',\n",
      " 'Continued',\n",
      " 'fully',\n",
      " 'expanded',\n",
      " 'midline',\n",
      " 'IJ',\n",
      " 'fluid',\n",
      " 'Monitoring',\n",
      " 'port',\n",
      " 'catheter',\n",
      " 'Hyperinflated',\n",
      " 'over',\n",
      " 'superior vena cava',\n",
      " 'intrathoracic',\n",
      " 'jugular',\n",
      " 'superior',\n",
      " 'cavoatrial',\n",
      " 'caliber',\n",
      " 'Port - A - Cath',\n",
      " 'cavoatrial junction',\n",
      " 'essentially',\n",
      " 'inflated',\n",
      " 'aortic',\n",
      " 'knuckle',\n",
      " 'demineralization',\n",
      " 'Well',\n",
      " 'focus',\n",
      " 'hazy',\n",
      " 'ill - defined',\n",
      " 'lingular',\n",
      " 'faint',\n",
      " 'perihilar',\n",
      " 'in extent',\n",
      " 'severity',\n",
      " 'Retrocardiac',\n",
      " 'valvular',\n",
      " 'Discrete',\n",
      " 'worsened',\n",
      " 'infrahilar',\n",
      " 'drain',\n",
      " 'internal jugular',\n",
      " 'postoperative',\n",
      " 'Midline',\n",
      " 'drains',\n",
      " 'in place',\n",
      " 'Swan - Ganz',\n",
      " 'pulmonic',\n",
      " 'nondistended',\n",
      " '5 cm above',\n",
      " 'Median',\n",
      " 'proximally',\n",
      " 'cardia',\n",
      " 'blurring',\n",
      " 'side port',\n",
      " 'the Swan - Ganz',\n",
      " 'homogeneous',\n",
      " 'worrisome',\n",
      " 'Trace',\n",
      " 'Biapical',\n",
      " 'Surgical',\n",
      " 'pneumomediastinum',\n",
      " 'biapical',\n",
      " 'surgical',\n",
      " 'deviation',\n",
      " 'thyroid',\n",
      " 'vessel on end',\n",
      " 'malignancy',\n",
      " 'Thoracic',\n",
      " 'RELEVANT',\n",
      " 'CHANGE',\n",
      " 'PARENCHYMAL',\n",
      " 'OPACITIES',\n",
      " 'PNEUMONIA',\n",
      " 'OTHER',\n",
      " 'FOCAL',\n",
      " 'DIFFUSE',\n",
      " 'LUNG',\n",
      " 'DISEASE',\n",
      " 'NORMAL',\n",
      " 'SIZE',\n",
      " 'CARDIAC',\n",
      " 'SILHOUETTE',\n",
      " 'HILAR',\n",
      " 'MEDIASTINAL',\n",
      " 'STRUCTURES',\n",
      " 'AZYGOS',\n",
      " 'LOBE',\n",
      " 'remainder',\n",
      " 'azygos',\n",
      " 'arch',\n",
      " 'midlung',\n",
      " 'Calcific',\n",
      " 'densities',\n",
      " 'crowding of',\n",
      " 'minimally',\n",
      " 'crowd',\n",
      " 'Hemidiaphragm',\n",
      " 'some',\n",
      " 'Blunting',\n",
      " 'in standard position',\n",
      " 'side holes',\n",
      " 'axillary',\n",
      " 'tips',\n",
      " '3.5 cm',\n",
      " 'object',\n",
      " 'components',\n",
      " 'curvilinear',\n",
      " 'beneath',\n",
      " 'mainstem',\n",
      " 'bronchus',\n",
      " 'related',\n",
      " 'well - expanded',\n",
      " 'in acceptable position',\n",
      " 'advanced',\n",
      " 'by several centimeters',\n",
      " 'implement',\n",
      " 'medially',\n",
      " 'less',\n",
      " 'a change in patient position',\n",
      " 'not well seen',\n",
      " 'Pacer',\n",
      " 'leads',\n",
      " 'less prominent',\n",
      " 'upright position',\n",
      " 'on waterseal',\n",
      " 'amount',\n",
      " 'Rib',\n",
      " 'partially assessed',\n",
      " 'tiny',\n",
      " 'appreciable',\n",
      " 'existing',\n",
      " 'thickenings',\n",
      " 'injury',\n",
      " 'regressed',\n",
      " 'gross',\n",
      " 'malalignment',\n",
      " 'lateral structures',\n",
      " 'thorax',\n",
      " 'elevate',\n",
      " 'injuries',\n",
      " 'transverse',\n",
      " 'L 2 through L 4',\n",
      " 'first',\n",
      " 'portions',\n",
      " 'comminuted',\n",
      " 'posteriorly',\n",
      " 'displacement',\n",
      " 'scapula',\n",
      " 'hemithorax',\n",
      " 'to the right',\n",
      " 'obliquity of the patient',\n",
      " 'shift',\n",
      " 'Poor definition',\n",
      " 'clavicle',\n",
      " 'not as well seen on this study',\n",
      " '4 cm above',\n",
      " 'lucency',\n",
      " 'emphysema',\n",
      " 'left - sided',\n",
      " 'Pneumomediastinum',\n",
      " 'severely',\n",
      " 'hemorrahge',\n",
      " 'assessment',\n",
      " 'limited',\n",
      " 'higher',\n",
      " 'opacified',\n",
      " 'alongside',\n",
      " 'elongated',\n",
      " 'aortosclerosis',\n",
      " 'increse',\n",
      " 'collections',\n",
      " 'bleeding',\n",
      " 'sharp',\n",
      " 'severe',\n",
      " 'subcutaneus',\n",
      " 'deeper',\n",
      " 'pneumo mediastinum',\n",
      " '1-2 mm',\n",
      " 'Unremarkable',\n",
      " 'tuberculous',\n",
      " 'A',\n",
      " 'rod',\n",
      " 'overlies',\n",
      " 'T 5 through T 9',\n",
      " 'of T 5 through T 9',\n",
      " 'intrafissural',\n",
      " '1.5 cm',\n",
      " 'aforementioned',\n",
      " 'excavatum',\n",
      " 'feeding',\n",
      " 'tubes',\n",
      " 'pneumothoraces',\n",
      " 'Swan',\n",
      " 'sheath',\n",
      " 'positions',\n",
      " 'lines',\n",
      " 'bubble',\n",
      " 'subpulmonic',\n",
      " 'Post - operative',\n",
      " 'predominance',\n",
      " 'markedly',\n",
      " 'Calcified',\n",
      " 'Streaky',\n",
      " 'diffusely',\n",
      " 'demineralized',\n",
      " 'vertebral',\n",
      " 'bodies',\n",
      " 'humeral',\n",
      " 'calcific',\n",
      " 'indistinctness',\n",
      " 'worsening',\n",
      " 'vessels',\n",
      " 'sinus',\n",
      " 'moderate - sized',\n",
      " 'peribronchial',\n",
      " 'interspace',\n",
      " 'non - tuberculous',\n",
      " 'poor',\n",
      " 'reexpansion',\n",
      " 'thoracocentesis',\n",
      " 'pre - existing',\n",
      " 'substantially',\n",
      " 'post - procedural',\n",
      " 'prior',\n",
      " 'bibasal',\n",
      " 'in size',\n",
      " 'Degree',\n",
      " 'lumbar',\n",
      " 'Enlarging',\n",
      " 'Perihilar',\n",
      " 'haziness',\n",
      " 'retracted 4.5 cm',\n",
      " 'pulled back about 5 cm',\n",
      " 'collapse',\n",
      " 'Feeding',\n",
      " 'stylet',\n",
      " 'superior cavoatrial junction',\n",
      " 'Subtle',\n",
      " 'minor',\n",
      " 'internal jugular central venous',\n",
      " 'associated exudate',\n",
      " 'reasonably',\n",
      " 'fluid level',\n",
      " 'Cephalization',\n",
      " 'Comminuted',\n",
      " 'setting of splinting',\n",
      " 'Partially imaged',\n",
      " 'fusion',\n",
      " 'hardware',\n",
      " 'subsegmental',\n",
      " 'Displaced',\n",
      " 'similar compared to the previous exams',\n",
      " 'especially',\n",
      " 'fixation',\n",
      " '3 cm above',\n",
      " 'transesophageal',\n",
      " 'developing',\n",
      " 'Developing',\n",
      " 'Support',\n",
      " 'in',\n",
      " 'noncardiogenic',\n",
      " 'sparing',\n",
      " 'periphery',\n",
      " 'significantly',\n",
      " 'Findings',\n",
      " 'stably',\n",
      " 'predominantly',\n",
      " 'plate - like',\n",
      " 'bones',\n",
      " 'platelike',\n",
      " 'a',\n",
      " 'Plate - like',\n",
      " 'extubated',\n",
      " '1',\n",
      " 'mucous',\n",
      " 'plug',\n",
      " 'lying',\n",
      " 'approximately 2.3 cm',\n",
      " '2',\n",
      " '3.5 cm above',\n",
      " 'ARDS',\n",
      " 'air bronchograms',\n",
      " '4.7 cm above',\n",
      " 'curled',\n",
      " 'catheters',\n",
      " 'satisfactory',\n",
      " '2.8 cm above',\n",
      " 'small - to - moderate',\n",
      " 'haze',\n",
      " 'Moderate - to - severe',\n",
      " 'moderate - to - severe',\n",
      " 'Indistinct',\n",
      " 'segment',\n",
      " 'Widened',\n",
      " 'out of view',\n",
      " 'denser',\n",
      " 'ground - glass',\n",
      " 'withdrawn 1 cm',\n",
      " '2.9 cm above',\n",
      " 'coiled',\n",
      " '3.4 cm',\n",
      " 'adenopathy',\n",
      " 'Expected',\n",
      " 'Little',\n",
      " 'extubation',\n",
      " 'NGT',\n",
      " 'removal',\n",
      " 'bronchograms',\n",
      " 'obliterating',\n",
      " '3.6 cm above',\n",
      " 'out of the field of view',\n",
      " 'Enlargement',\n",
      " 'AP technique',\n",
      " 'cardiomegally',\n",
      " 'right side',\n",
      " 'inlet',\n",
      " 'secretions',\n",
      " 'pacer',\n",
      " 'not well evaluated',\n",
      " 'cage',\n",
      " 'gas',\n",
      " 'under',\n",
      " 'subdiaphragmatic',\n",
      " 'approximately',\n",
      " '13 mm',\n",
      " 'hyperlucent',\n",
      " 'Fully',\n",
      " 'underpenetration',\n",
      " 'Worsening',\n",
      " 'supine',\n",
      " 'great',\n",
      " 'vessel',\n",
      " '6 cm above',\n",
      " 'coursing',\n",
      " 'inferiorly',\n",
      " 'expected',\n",
      " 'widened',\n",
      " 'positioned',\n",
      " '7 cm',\n",
      " 'fat',\n",
      " 'type',\n",
      " 'juxta',\n",
      " 'nasoenteric',\n",
      " 'GE junction',\n",
      " 'semiupright',\n",
      " 'projection',\n",
      " 'Its',\n",
      " 'gastric',\n",
      " 'drainage',\n",
      " 'quite',\n",
      " 'milder',\n",
      " 'AICD',\n",
      " 'single',\n",
      " 'Indistinctness',\n",
      " 'procedure',\n",
      " 'resection',\n",
      " 'nodular',\n",
      " 'suture',\n",
      " 'Remainder',\n",
      " 'chain',\n",
      " 'sutures',\n",
      " 'Free',\n",
      " 'underneath',\n",
      " 'projectional',\n",
      " 'ill',\n",
      " 'Chain',\n",
      " 'Rounded',\n",
      " 'discrete',\n",
      " 'suprahilar',\n",
      " 'consolidative',\n",
      " 'longstanding',\n",
      " 'micronodules',\n",
      " 'thoracolumbar junction',\n",
      " 'Opacity',\n",
      " 'not appreciably changed',\n",
      " 'bulging',\n",
      " 'Band - like',\n",
      " 'Post - surgical',\n",
      " 'Nodular',\n",
      " 'wedge resection',\n",
      " 'Subcentimeter',\n",
      " 'it',\n",
      " 'amorphous',\n",
      " 'Suture',\n",
      " 'sarcoid',\n",
      " 'flair',\n",
      " 'compartment',\n",
      " 'Tiny',\n",
      " 'high - positioned',\n",
      " 'plate',\n",
      " 'sinuses',\n",
      " 'borders',\n",
      " 'overlie',\n",
      " 'posteroinferior',\n",
      " 'Substantial',\n",
      " 'Vertebral',\n",
      " 'Tubes',\n",
      " 'fine',\n",
      " 'layered',\n",
      " 'hazinness',\n",
      " 'Layered',\n",
      " '8 cm above',\n",
      " 'nondisplaced',\n",
      " 'dorsolateral part',\n",
      " 'masses',\n",
      " 'Asymmetric',\n",
      " 'Pre',\n",
      " 'manifestation',\n",
      " 'S - shaped',\n",
      " 'silhouetting',\n",
      " 'decompensating',\n",
      " 'cardiogenic',\n",
      " 'Scoliosis',\n",
      " 'SUBSTANTIAL',\n",
      " 'ENLARGEMENT',\n",
      " 'CONTINUED',\n",
      " 'PULMONARY',\n",
      " 'EDEMA',\n",
      " 'BILATERAL',\n",
      " 'PLEURAL',\n",
      " 'EFFUSIONS',\n",
      " 'COMPRESSIVE',\n",
      " 'BASILAR',\n",
      " 'ATELECTASIS',\n",
      " 'significant interval',\n",
      " 'Hazy',\n",
      " 'Increasing',\n",
      " 'improving',\n",
      " 'peripherally',\n",
      " 'inserted',\n",
      " 'kyphotic',\n",
      " 'curvature',\n",
      " 'Hyperinflation',\n",
      " 'Layering',\n",
      " 'bigger',\n",
      " '2.3 cm above the carina',\n",
      " 'Dobbhoff',\n",
      " 'bowel',\n",
      " 'PIC',\n",
      " 'brachiocephalic',\n",
      " 'pigtail',\n",
      " '4.6 cm above',\n",
      " 'air space',\n",
      " 'pronounced',\n",
      " 'continued',\n",
      " 'supervening',\n",
      " 'Superiorly',\n",
      " 'pleural - based',\n",
      " 'sided',\n",
      " 'approximately 5 cm',\n",
      " '5.6 cm',\n",
      " 'above',\n",
      " 'ports',\n",
      " '1.8 cm above',\n",
      " 'sidehole',\n",
      " 'costal',\n",
      " 'heavily',\n",
      " 'mid - to - distal',\n",
      " '20 cm for more optimal placement',\n",
      " 'which',\n",
      " 'Pigtail',\n",
      " 'pnemonia',\n",
      " 'Hila',\n",
      " 'indistinct',\n",
      " 'collection',\n",
      " 'cavitary',\n",
      " 'OG',\n",
      " 'ETT',\n",
      " 'postion',\n",
      " 'Dahboff',\n",
      " 'plerual',\n",
      " 'Distal',\n",
      " 'hyperinflation',\n",
      " 'Mild - to - moderate',\n",
      " 'mild - to - moderate',\n",
      " 'triple',\n",
      " 'no change',\n",
      " 'pneumonic',\n",
      " 'outer margin',\n",
      " 'Triple',\n",
      " 'in the expected positions',\n",
      " 'cardiomyopathy',\n",
      " 'Position',\n",
      " 'radiopaque',\n",
      " 'one - third',\n",
      " '1 cm',\n",
      " 'mass',\n",
      " 'glenohumeral',\n",
      " 'Minimal to no',\n",
      " 'The right - sided',\n",
      " 'Multilevel',\n",
      " 'Obscuration',\n",
      " 'Operation',\n",
      " 'angles',\n",
      " 'not well visualized',\n",
      " 'not increased',\n",
      " 'No change',\n",
      " 'sized',\n",
      " 'It',\n",
      " 'rounded',\n",
      " 'Opacities',\n",
      " 'Emphysema',\n",
      " 'hyperexpansion',\n",
      " 'T 12 vertebral',\n",
      " 'opportunistic',\n",
      " 'reaction to inhaled substances',\n",
      " 'scapular',\n",
      " 'metastatic',\n",
      " 'callus',\n",
      " 'lucent',\n",
      " 'centers',\n",
      " 'intersitial',\n",
      " 'similar to prior radiographs',\n",
      " 'hyperexpanded',\n",
      " 'evidence of prior smoking',\n",
      " 'configurational',\n",
      " 'semi - linear',\n",
      " 'deposits',\n",
      " 'hypertranslucent',\n",
      " 'lucencies',\n",
      " 'wedging',\n",
      " 'processes',\n",
      " 'lymphomatous',\n",
      " 'replacements',\n",
      " 'waterseal',\n",
      " 'takeoff',\n",
      " 'regions',\n",
      " 'deviated',\n",
      " 'sequela',\n",
      " 'amount of',\n",
      " 'AC',\n",
      " 'Overlying',\n",
      " 'EKG',\n",
      " 'Minimally',\n",
      " 'Bronchovascular',\n",
      " 'separation']\n"
     ]
    }
   ],
   "source": [
    "pprint(entities_null_normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to ../output.csv\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "\n",
    "# # Specify the file path\n",
    "# csv_file_path = '../output.csv'\n",
    "\n",
    "# # Extract the required fields from all_entities\n",
    "# data = []\n",
    "\n",
    "# for key, value in all_entities.items():\n",
    "#     name = key\n",
    "#     ui = None\n",
    "#     definition = None\n",
    "#     semanticTypes = None\n",
    "#     normalized_name = None\n",
    "\n",
    "#     if value.get('normalization') and value['normalization'].get('UMLS'):\n",
    "#         umls = value['normalization']['UMLS']\n",
    "#         ui = umls.get('ui')\n",
    "#         normalized_name = umls.get('name')\n",
    "#         semanticTypes = umls.get('semanticTypes')\n",
    "#         definition = umls.get('definition', {}).get('definition')\n",
    "\n",
    "#     data.append({\n",
    "#         'name': name,\n",
    "#         'ui': ui,\n",
    "#         'normalized_name': normalized_name,\n",
    "#         'semanticTypes': semanticTypes,\n",
    "#         'definition': definition\n",
    "#     })\n",
    "\n",
    "\n",
    "# # Write the data to the CSV file\n",
    "# with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#     fieldnames = ['name', 'ui', 'normalized_name', 'semanticTypes', 'definition']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(data)\n",
    "\n",
    "# print(f\"Data has been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to ../gpt_normalization.csv\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "\n",
    "# # Specify the file path\n",
    "# csv_file_path = '../gpt_normalization.csv'\n",
    "\n",
    "\n",
    "\n",
    "# import json\n",
    "# from pprint import pprint\n",
    "\n",
    "# file_path = '../resource/all_unique_entities_normalized.json'\n",
    "\n",
    "# # Read the JSON file\n",
    "# with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#     all_entities = json.load(f)\n",
    "\n",
    "# # get list of all entities with normalization: null\n",
    "# entities_null_normalization = [entity for entity, data in all_entities.items() if data['normalization'] is None]\n",
    "\n",
    "\n",
    "# data = []\n",
    "\n",
    "# import sys\n",
    "# import os\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Add the parent directory to the Python path\n",
    "# parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "# sys.path.append(parent_dir)\n",
    "\n",
    "# from utils.umls_api import normalize_entity \n",
    "# from utils.umls_api import use_umls_api_term\n",
    "\n",
    "# for entity in entities_null_normalization:\n",
    "    \n",
    "#     response = use_umls_api_term(entity)\n",
    "#     result_list = response['result']['results'][:5]\n",
    "\n",
    "#     ui, name = normalize_entity(entity, result_list)\n",
    "\n",
    "#     data.append({\n",
    "#         'name': entity,\n",
    "#         'ui': ui,\n",
    "#         'normalized_name': name,\n",
    "#     })\n",
    "\n",
    "\n",
    "\n",
    "# # Write the data to the CSV file\n",
    "# with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#     fieldnames = ['name', 'ui', 'normalized_name', 'semanticTypes', 'definition']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(data)\n",
    "\n",
    "# print(f\"Data has been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the file path\n",
    "csv_file_path = '../gpt_normalization.csv'\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Print the data\n",
    "# print(data)\n",
    "\n",
    "\n",
    "count_normalizable = sum(1 for entry in data if entry['ui'] != 'unnormalizable')\n",
    "print(f\"Number of entries with 'normalizable' ui: {count_normalizable}\")\n",
    "\n",
    "\n",
    "normalizable = [entry for entry in data if entry['ui'] != 'unnormalizable']\n",
    "for entry in normalizable:\n",
    "    print(entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis after using the normalization with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entities: 1250\n",
      "Number of entities with normalization: null: 752\n",
      "Number of entries with 'normalizable' ui: 390\n",
      "we still have 362 entities that are not normalizable\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "file_path = '../resource/all_unique_entities_normalized.json'\n",
    "\n",
    "# Read the JSON file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    all_entities = json.load(f)\n",
    "\n",
    "# Print total number of entities\n",
    "print(f\"Total number of entities: {len(all_entities)}\")\n",
    "# get list of all entities with normalization: null\n",
    "entities_null_normalization = [entity for entity, data in all_entities.items() if data['normalization'] is None]\n",
    "# len(entities_null_normalization)\n",
    "print(f\"Number of entities with normalization: null: {len(entities_null_normalization)}\")\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "# Specify the file path\n",
    "csv_file_path = '../resource/gpt_normalization.csv'\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "\n",
    "count_normalizable = sum(1 for entry in data if entry['ui'] != 'unnormalizable')\n",
    "print(f\"Number of entries with 'normalizable' ui: {count_normalizable}\")\n",
    "\n",
    "\n",
    "# we have stiil 752 - 390 = 362 entities that are not normalizable\n",
    "print(f\"we still have {len(entities_null_normalization) - count_normalizable} entities that are not normalizable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with 'None' ui: 362\n",
      "we have 888 entities that are normalizable\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = '../resource/output.csv'\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# # check how many entities have no ui\n",
    "# count_no_ui = sum(1 for entry in data if entry['ui'] == '')\n",
    "# print(f\"Number of entries with 'None' ui: {count_no_ui}\")\n",
    "\n",
    "no_ui = [entry for entry in data if entry['ui'] == '']\n",
    "# for entry in no_ui:\n",
    "    # print(entry)\n",
    "\n",
    "\n",
    "print(f\"Number of entries with 'None' ui: {len(no_ui)}\")\n",
    "# total 1250\n",
    "# we have 1250 - 390 = 860 entities that are normalizable\n",
    "print(f\"we have {len(data) - len(no_ui)} entities that are normalizable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../resource/not_normalized.csv'\n",
    "\n",
    "# Write the data to the CSV file\n",
    "with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['name', 'ui', 'normalized_name', 'semanticTypes', 'definition']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(no_ui)\n",
    "\n",
    "print(f\"Data has been written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "file_path = '../resource/output.csv'\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# add a new column to the data, report.\n",
    "for entry in data:\n",
    "    entry['report'] = ''\n",
    "\n",
    "    # ../../resource/all_unique_entities.json\n",
    "    # Read the JSON file\n",
    "    with open('../resource/all_unique_entities.json', 'r', encoding='utf-8') as f:\n",
    "        all_entities = json.load(f)\n",
    "\n",
    "    # all_entities[entry['name']]['reports'][0] is a dict, get the key\n",
    "    report_file = all_entities[entry['name']]['reports'][0].keys()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to ../resource/output_with_report.csv\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "# import json\n",
    "# from pprint import pprint\n",
    "\n",
    "# file_path = '../resource/output.csv'\n",
    "\n",
    "# # Read the data from the CSV file\n",
    "# data = []\n",
    "# with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "#     for row in reader:\n",
    "#         data.append(row)\n",
    "\n",
    "# path = '/DATA1/llm-research/RadGraph/physionet.org/files/radgraph/1.0.0/train.json'\n",
    "\n",
    "# with open(path, 'r') as f:\n",
    "#     train_data = json.load(f)\n",
    "\n",
    "# # ../../resource/all_unique_entities.json\n",
    "# # Read the JSON file\n",
    "# with open('../../resource/all_unique_entities.json', 'r', encoding='utf-8') as f:\n",
    "#     all_entities = json.load(f)\n",
    "\n",
    "\n",
    "# # add a new column to the data, report.\n",
    "# # entry = data[0]\n",
    "# for entry in data:\n",
    "#     entry['report'] = ''\n",
    "\n",
    "#     # all_entities[entry['name']]['reports'][0] is a dict, get the key\n",
    "#     report_file = list(all_entities[entry['name']]['reports'][0].keys())[0]\n",
    "\n",
    "#     entry['report'] = train_data[report_file]['text']\n",
    "\n",
    "# file_path = '../resource/output_with_report.csv'\n",
    "\n",
    "\n",
    "# # write the data back to the CSV file\n",
    "# with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#     fieldnames = ['name', 'ui', 'normalized_name', 'semanticTypes', 'definition', 'report']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(data)\n",
    "\n",
    "# print(f\"Data has been written to {file_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with 'unnormalizable' ui: 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "file_path = '../resource/output.csv'\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# count the number of entities with no normalization\n",
    "count_no_norm = sum(1 for entry in data if entry['ui'] == 'unnormalizable') \n",
    "print(f\"Number of entries with 'unnormalizable' ui: {count_no_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to output.xlsx\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the data from the CSV file\n",
    "# data = pd.read_csv('../resource/output.csv')\n",
    "\n",
    "# # Write the data to an Excel file\n",
    "# data.to_excel('../resource/output.xlsx', index=False)\n",
    "\n",
    "# print(\"Data has been written to output.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subt set of entities e.g. first 199 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to output_201_400.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first 200 data from the CSV file\n",
    "data = pd.read_csv('../resource/output_with_report.csv')\n",
    "\n",
    "# print(data[200:400])\n",
    "\n",
    "# Write the first 201-400 data to an Excel file\n",
    "# data.head(200).to_excel('../resource/output_200.xlsx', index=False)\n",
    "data[200:400].to_excel('../resource/output_201_400.xlsx', index=False)\n",
    "\n",
    "\n",
    "# print(\"Data has been written to output_200.xlsx\")\n",
    "print(\"Data has been written to output_201_400.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the first results from radiology expert\n",
    "105/200, for sure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities with 'Radiology Expert verify' == 'Yes': 122\n",
      "Number of entities with 'Radiology Expert verify' == 'No': 1\n",
      "Number of entities with 'Radiology Expert verify' == NAN: 76\n",
      "\n",
      "Number of entities with 'Radiology Expert verify' == 'Yes' and 'Comment' == NAN: 105\n",
      "\n",
      "This is very confident data\n",
      "Number of entities with 'Comment' != NAN: 48\n",
      "Number of entities with 'Radiology Expert verify' == 'No': 1\n",
      "Number of entities with 'Radiology Expert verify' == NAN: 76\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "data = pd.read_excel('../resource/output_200-verified_hb.xlsx')\n",
    "\n",
    "# Print the data\n",
    "# print(data)\n",
    "\n",
    "\n",
    "# data with YES\n",
    "subset_data = data[data['Radiology Expert verify'] == 'Yes']\n",
    "print(f\"Number of entities with 'Radiology Expert verify' == 'Yes': {len(subset_data)}\")\n",
    "# data with NO\n",
    "subset_data = data[data['Radiology Expert verify'] == 'No']\n",
    "print(f\"Number of entities with 'Radiology Expert verify' == 'No': {len(subset_data)}\")\n",
    "# extract subset of data, 'Radiology Expert verify'.isnull()\n",
    "subset_data = data[data['Radiology Expert verify'].isnull()]\n",
    "print(f\"Number of entities with 'Radiology Expert verify' == NAN: {len(subset_data)}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# extract subset of data, 'Radiology Expert verify' == 'YES' and 'Comment' == NAN\n",
    "subset_data = data[(data['Radiology Expert verify'] == 'Yes') & (data['Comment'].isnull())]\n",
    "# print(len(subset_data))\n",
    "print(f\"Number of entities with 'Radiology Expert verify' == 'Yes' and 'Comment' == NAN: {len(subset_data)}\\n\")\n",
    "print(\"This is very confident data\")\n",
    "\n",
    "\n",
    "\n",
    "# data with comment\n",
    "subset_data = data[data['Comment'].notnull()]\n",
    "print(f\"Number of entities with 'Comment' != NAN: {len(subset_data)}\")\n",
    "\n",
    "\n",
    "# data with NO\n",
    "subset_data = data[data['Radiology Expert verify'] == 'No']\n",
    "print(f\"Number of entities with 'Radiology Expert verify' == 'No': {len(subset_data)}\")\n",
    "\n",
    "\n",
    "# extract subset of data, 'Radiology Expert verify'.isnull()\n",
    "subset_data = data[data['Radiology Expert verify'].isnull()]\n",
    "print(f\"Number of entities with 'Radiology Expert verify' == NAN: {len(subset_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities with 'Radiology Expert verify' != 'Yes' or 'Comment' != NAN: 95\n"
     ]
    }
   ],
   "source": [
    "# extract subset of data, 'Radiology Expert verify' != 'YES' or 'Comment' != NAN\n",
    "subset_data = data[(data['Radiology Expert verify'] != 'Yes') | (data['Comment'].notnull())]\n",
    "print(f\"Number of entities with 'Radiology Expert verify' != 'Yes' or 'Comment' != NAN: {len(subset_data)}\")\n",
    "\n",
    "# wirte the data to a new Excel file\n",
    "subset_data.to_excel('../resource/output_200_review.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medkgc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
