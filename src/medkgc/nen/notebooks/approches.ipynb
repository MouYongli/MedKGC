{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read MRCONSO.RRF file\n",
    "## Target subset of UMLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411747\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# 定义文件路径  /DATA1/llm-research/2022AA-full/\n",
    "# mrconso_path = '../resource/Radiology/2024AA/META/MRCONSO.RRF'\n",
    "mrconso_path = '/Users/hanbin/Downloads/2024AA-full/0827/0827/2024AA/META/MRCONSO.RRF'\n",
    "\n",
    "concepts = {}\n",
    "\n",
    "with open(mrconso_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter='|')\n",
    "    for row in reader:\n",
    "        cui = row[0]\n",
    "        concept_name = row[14]\n",
    "        concepts[cui] = concept_name\n",
    "\n",
    "print(len(concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C0000696 a fiber\n",
      "C0000726 Abdominal structure (body structure)\n",
      "C0000727 acute abdomen (diagnosis)\n",
      "C0000729 [D]Abdominal cramps (situation)\n",
      "C0000731 abdominal distention (physical finding)\n",
      "C0000734 Abdominal mass (disorder)\n",
      "C0000735 ABDOMINAL NEOPL\n",
      "C0000737 Unspecified abdominal pain\n",
      "C0000739 Musculature of abdomen\n",
      "C0000741 Abducens nerve structure (body structure)\n"
     ]
    }
   ],
   "source": [
    "# first 10 concepts\n",
    "for cui, name in list(concepts.items())[:10]:\n",
    "    print(cui, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: lung\n",
      "1. C0746117: mass left lung\n",
      "2. C1278908: Entire lung (body structure)\n",
      "3. C0024109: Structure of lungs, unspecified\n",
      "4. C4037972: Lung\n",
      "5. C2200156: lung tissue swab of left lung (lab test)\n",
      "\n",
      "Query: Normal\n",
      "1. C1553399: normal\n",
      "2. C1704701: Normality-Based Dosing Unit\n",
      "3. C1550457: Normal\n",
      "4. C0205307: Normal (qualifier value)\n",
      "5. C1553406: EntityStatusNormal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# 读取MRCONSO.RRF文件\n",
    "mrconso_path = '/Users/hanbin/Downloads/2024AA-full/0827/0827/2024AA/META/MRCONSO.RRF'\n",
    "df = pd.read_csv(mrconso_path, sep='|', header=None, usecols=[0, 14], names=['CUI', 'STR'])\n",
    "\n",
    "# 去重\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 创建CUI到STR的映射\n",
    "cui_to_str = df.set_index('CUI')['STR'].to_dict()\n",
    "\n",
    "# 创建语料库，确保所有值都是字符串\n",
    "corpus = df['STR'].astype(str).tolist()\n",
    "\n",
    "# 对语料库进行分词，同时处理可能的非字符串值\n",
    "def tokenize(doc):\n",
    "    return str(doc).lower().split()\n",
    "\n",
    "tokenized_corpus = [tokenize(doc) for doc in corpus]\n",
    "\n",
    "# 创建BM25模型\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def get_top_n_candidates(query, n=5):\n",
    "    tokenized_query = tokenize(query)\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # 使用OrderedDict来去重，同时保持顺序\n",
    "    results = OrderedDict()\n",
    "    for idx in np.argsort(doc_scores)[::-1]:\n",
    "        cui = df.iloc[idx]['CUI']\n",
    "        if cui not in results and len(results) < n:\n",
    "            results[cui] = cui_to_str[cui]\n",
    "    \n",
    "    return list(results.items())\n",
    "\n",
    "# 测试函数\n",
    "test_queries = [\n",
    "    \"abdominal pain\",\n",
    "    \"heart failure\",\n",
    "    \"lung cancer\",\n",
    "    \"diabetes\"\n",
    "]\n",
    "\n",
    "# test 'lung'\n",
    "test_queries = [\n",
    "    \"lung\",\n",
    "    \"Normal\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    candidates = get_top_n_candidates(query)\n",
    "    for i, (cui, str_value) in enumerate(candidates, 1):\n",
    "        print(f\"{i}. {cui}: {str_value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置您的OpenAI API密钥\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# 读取MRCONSO.RRF文件\n",
    "df = pd.read_csv('MRCONSO.RRF', sep='|', header=None, usecols=[0, 14], names=['CUI', 'STR'])\n",
    "\n",
    "# 去重\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 创建CUI到STR的映射\n",
    "cui_to_str = df.set_index('CUI')['STR'].to_dict()\n",
    "\n",
    "# 创建语料库，确保所有值都是字符串\n",
    "corpus = df['STR'].astype(str).tolist()\n",
    "\n",
    "# 对语料库进行分词，同时处理可能的非字符串值\n",
    "def tokenize(doc):\n",
    "    return str(doc).lower().split()\n",
    "\n",
    "tokenized_corpus = [tokenize(doc) for doc in corpus]\n",
    "\n",
    "# 创建BM25模型\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def get_top_n_candidates(query, n=5):\n",
    "    tokenized_query = tokenize(query)\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # 使用OrderedDict来去重，同时保持顺序\n",
    "    results = OrderedDict()\n",
    "    for idx in np.argsort(doc_scores)[::-1]:\n",
    "        cui = df.iloc[idx]['CUI']\n",
    "        if cui not in results and len(results) < n:\n",
    "            results[cui] = cui_to_str[cui]\n",
    "    \n",
    "    return list(results.items())\n",
    "\n",
    "def normalize_entity(entity, results, report_context=\"\", myModel=\"gpt-3.5-turbo\"):\n",
    "    client = OpenAI()\n",
    "\n",
    "    results_str = json.dumps(results, indent=2)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in named entity normalization for medical terms using the UMLS ontology. Your task is to analyze the given entity and search results, then select the most appropriate normalized form or the most likely UMLS concept.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Entity: {entity}\\n\\nSearch Results:\\n{results_str}\\n\\nReport Context: {report_context}\\n\\nBased on these results and the report context, provide the most appropriate CUI and name for this entity. If there's no exact match, provide the most likely UMLS concept. If the entity is unlikely to be normalized, return ('unnormalizable', 'unnormalizable'). Respond in the format: (cui, name)\"}\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=myModel,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content.strip()\n",
    "    try:\n",
    "        cui, name = eval(response)\n",
    "    except:\n",
    "        cui, name = \"unnormalizable\", \"unnormalizable\"\n",
    "\n",
    "    return cui, name\n",
    "\n",
    "# 示例使用\n",
    "test_queries = [\n",
    "    \"abdominal pain\",\n",
    "    \"heart failure\",\n",
    "    \"lung cancer\",\n",
    "    \"diabetes\"\n",
    "]\n",
    "\n",
    "# 模拟报告上下文\n",
    "report_context = \"Patient presents with severe abdominal pain and has a history of heart disease.\"\n",
    "\n",
    "for query in tqdm(test_queries):\n",
    "    print(f\"Query: {query}\")\n",
    "    candidates = get_top_n_candidates(query)\n",
    "    cui, name = normalize_entity(query, candidates, report_context)\n",
    "    print(f\"Normalized: {cui}: {name}\")\n",
    "    print()\n",
    "\n",
    "# 批量处理实体\n",
    "def process_entities(entities, report_context=\"\"):\n",
    "    results = []\n",
    "    for entity in tqdm(entities):\n",
    "        candidates = get_top_n_candidates(entity)\n",
    "        cui, name = normalize_entity(entity, candidates, report_context)\n",
    "        results.append((entity, cui, name))\n",
    "    return results\n",
    "\n",
    "# 示例批量处理\n",
    "entities = [\"hypertension\", \"myocardial infarction\", \"asthma\", \"pneumonia\"]\n",
    "batch_results = process_entities(entities, report_context)\n",
    "\n",
    "# 显示批量处理结果\n",
    "for entity, cui, name in batch_results:\n",
    "    print(f\"Entity: {entity}\")\n",
    "    print(f\"Normalized: {cui}: {name}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Levenshtein'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLevenshtein\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance \u001b[38;5;28;01mas\u001b[39;00m levenshtein_distance\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 下载NLTK的停用词\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Levenshtein'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "# 下载NLTK的停用词\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 读取MRCONSO.RRF文件\n",
    "# 假设文件结构为：CUI|LAT|TS|LUI|STT|SUI|ISPREF|AUI|SAUI|SCUI|SDUI|SAB|TTY|CODE|STR|SRL|SUPPRESS|CVF\n",
    "df = pd.read_csv('MRCONSO.RRF', sep='|', header=None, usecols=[0, 14], names=['cui', 'term'])\n",
    "\n",
    "# 只保留英语术语\n",
    "df = df[df.iloc[:, 1] == 'ENG']\n",
    "\n",
    "# 数据预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    # 移除特殊字符\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # 移除停用词\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # 词干提取\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# 应用预处理\n",
    "df['processed_term'] = df['term'].apply(preprocess_text)\n",
    "\n",
    "# 创建一个词汇表\n",
    "corpus = df['processed_term'].tolist()\n",
    "\n",
    "# 创建一个CountVectorizer对象\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split())\n",
    "\n",
    "# 将corpus转换为词频矩阵\n",
    "term_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 创建BM25Okapi对象\n",
    "bm25 = BM25Okapi(term_matrix)\n",
    "\n",
    "def fuzzy_match(query, term, max_distance=2):\n",
    "    \"\"\"使用Levenshtein距离进行模糊匹配\"\"\"\n",
    "    query_words = query.split()\n",
    "    term_words = term.split()\n",
    "    \n",
    "    if len(query_words) != len(term_words):\n",
    "        return False\n",
    "    \n",
    "    for q_word, t_word in zip(query_words, term_words):\n",
    "        if levenshtein_distance(q_word, t_word) > max_distance:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def search_umls(query, top_k=5, fuzzy=True):\n",
    "    # 预处理查询\n",
    "    processed_query = preprocess_text(query)\n",
    "    \n",
    "    # 使用BM25模型计算相似度得分\n",
    "    scores = bm25.get_scores(processed_query.split())\n",
    "    \n",
    "    # 获取前top_k*2个结果的索引（我们获取更多的结果，以便在应用模糊匹配后仍有足够的候选项）\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k*2]\n",
    "    \n",
    "    # 应用模糊匹配（如果启用）\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        term = df.iloc[idx]['term']\n",
    "        if not fuzzy or fuzzy_match(query, term):\n",
    "            results.append({\n",
    "                'cui': df.iloc[idx]['cui'],\n",
    "                'term': term,\n",
    "                'score': scores[idx]\n",
    "            })\n",
    "        if len(results) == top_k:\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 测试搜索函数\n",
    "test_query = \"lung\"\n",
    "results = search_umls(test_query)\n",
    "\n",
    "print(f\"Search results for '{test_query}':\")\n",
    "for result in results:\n",
    "    print(f\"CUI: {result['cui']}, Term: {result['term']}, Score: {result['score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medkgc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
